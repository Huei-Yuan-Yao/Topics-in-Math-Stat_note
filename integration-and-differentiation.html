<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.2 Integration and Differentiation | Topics in Mathematical Statistics</title>
  <meta name="description" content="This is my note of topics in mathematical statistics (under writing)." />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="2.2 Integration and Differentiation | Topics in Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is my note of topics in mathematical statistics (under writing)." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.2 Integration and Differentiation | Topics in Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is my note of topics in mathematical statistics (under writing)." />
  

<meta name="author" content="Huei-Yuan Yao" />


<meta name="date" content="2023-01-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-spaces-and-random-elements.html"/>
<link rel="next" href="conditional-expectation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Topics in Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prob.html"><a href="prob.html"><i class="fa fa-check"></i><b>2</b> Measure-based Probability theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html"><i class="fa fa-check"></i><b>2.1</b> Probability Spaces and Random Elements</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html#recap-measure-theory"><i class="fa fa-check"></i><b>2.1.1</b> Recap: Measure theory</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html#measurable-functions"><i class="fa fa-check"></i><b>2.1.2</b> Measurable Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html"><i class="fa fa-check"></i><b>2.2</b> Integration and Differentiation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html#lebesgue-integral"><i class="fa fa-check"></i><b>2.2.1</b> Lebesgue integral</a></li>
<li class="chapter" data-level="2.2.2" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html#radon-nikodym-derivative"><i class="fa fa-check"></i><b>2.2.2</b> Radon-Nikodym Derivative</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html"><i class="fa fa-check"></i><b>2.3</b> Conditional Expectation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-expectation-1"><i class="fa fa-check"></i><b>2.3.1</b> Conditional Expectation</a></li>
<li class="chapter" data-level="2.3.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#independence"><i class="fa fa-check"></i><b>2.3.2</b> Independence</a></li>
<li class="chapter" data-level="2.3.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-distribution"><i class="fa fa-check"></i><b>2.3.3</b> Conditional Distribution</a></li>
<li class="chapter" data-level="2.3.4" data-path="conditional-expectation.html"><a href="conditional-expectation.html#markov-chains-and-martingales"><i class="fa fa-check"></i><b>2.3.4</b> Markov chains and Martingales</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html"><i class="fa fa-check"></i><b>2.4</b> Asymptotical Theory</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#convergence-modes"><i class="fa fa-check"></i><b>2.4.1</b> Convergence modes</a></li>
<li class="chapter" data-level="2.4.2" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.4.2</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="2.4.3" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.4.3</b> Central Limit Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-decision-theory.html"><a href="statistical-decision-theory.html"><i class="fa fa-check"></i><b>3</b> Statistical Decision Theory</a>
<ul>
<li class="chapter" data-level="3.1" data-path="general-framework.html"><a href="general-framework.html"><i class="fa fa-check"></i><b>3.1</b> General Framework</a></li>
<li class="chapter" data-level="3.2" data-path="point-estimators.html"><a href="point-estimators.html"><i class="fa fa-check"></i><b>3.2</b> Point Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="point-estimators.html"><a href="point-estimators.html#umvue"><i class="fa fa-check"></i><b>3.2.1</b> UMVUE</a></li>
<li class="chapter" data-level="3.2.2" data-path="point-estimators.html"><a href="point-estimators.html#u-statistics"><i class="fa fa-check"></i><b>3.2.2</b> U-Statistics</a></li>
<li class="chapter" data-level="3.2.3" data-path="point-estimators.html"><a href="point-estimators.html#v-statistics"><i class="fa fa-check"></i><b>3.2.3</b> V-Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#ump-tests"><i class="fa fa-check"></i><b>3.3.1</b> UMP Tests</a></li>
<li class="chapter" data-level="3.3.2" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#tests-in-parametric-models"><i class="fa fa-check"></i><b>3.3.2</b> Tests in Parametric Models</a></li>
<li class="chapter" data-level="3.3.3" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#tests-in-nonparametric-models"><i class="fa fa-check"></i><b>3.3.3</b> Tests in Nonparametric Models</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-sets.html"><a href="confidence-sets.html"><i class="fa fa-check"></i><b>3.4</b> Confidence Sets</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-in-parametric-models.html"><a href="estimation-in-parametric-models.html"><i class="fa fa-check"></i><b>4</b> Estimation in Parametric Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bayes-estimators.html"><a href="bayes-estimators.html"><i class="fa fa-check"></i><b>4.1</b> Bayes Estimators</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="bayes-estimators.html"><a href="bayes-estimators.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayes-estimators.html"><a href="bayes-estimators.html#asymptotic-efficiency-of-bayes-estimators"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic efficiency of Bayes Estimators</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html"><i class="fa fa-check"></i><b>4.2</b> Method of Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#mle"><i class="fa fa-check"></i><b>4.2.1</b> MLE</a></li>
<li class="chapter" data-level="4.2.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#quasi-likelihoods-and-conditional-likelihoods"><i class="fa fa-check"></i><b>4.2.2</b> Quasi-likelihoods and conditional likelihoods</a></li>
<li class="chapter" data-level="4.2.3" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic efficiency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation-in-nonparametric-models.html"><a href="estimation-in-nonparametric-models.html"><i class="fa fa-check"></i><b>5</b> Estimation in Nonparametric Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="distributional-estimators.html"><a href="distributional-estimators.html"><i class="fa fa-check"></i><b>5.1</b> Distributional Estimators</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-functionals.html"><a href="statistical-functionals.html"><i class="fa fa-check"></i><b>5.2</b> Statistical Functionals</a></li>
<li class="chapter" data-level="5.3" data-path="sample-quantiles.html"><a href="sample-quantiles.html"><i class="fa fa-check"></i><b>5.3</b> Sample quantiles</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Estimating Equations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#framework"><i class="fa fa-check"></i><b>5.4.1</b> Framework</a></li>
<li class="chapter" data-level="5.4.2" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#consistency"><i class="fa fa-check"></i><b>5.4.2</b> Consistency</a></li>
<li class="chapter" data-level="5.4.3" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#asymptotic-normality"><i class="fa fa-check"></i><b>5.4.3</b> Asymptotic normality</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="generalized-methods-of-moments.html"><a href="generalized-methods-of-moments.html"><i class="fa fa-check"></i><b>5.5</b> Generalized Methods of Moments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Topics in Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="integration-and-differentiation" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Integration and Differentiation<a href="integration-and-differentiation.html#integration-and-differentiation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="lebesgue-integral" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Lebesgue integral<a href="integration-and-differentiation.html#lebesgue-integral" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An usual way to define Lebesgue integral is from simple function to non-negative function by approximation property, then to a general function by an easy decomposition. Let us start from simple function. Assume <span class="math inline">\(\phi=\sum_\limits{i=1}^k a_iI_{A_i}\)</span>, <span class="math inline">\(A_i\)</span>’s are disjoint. Then its integral with respect to measure <span class="math inline">\(\nu\)</span> is <span class="math display">\[\int \phi d\nu=\sum_\limits{i=1}^k a_i\nu(A_i).\]</span> Clearly <span class="math inline">\(A_i\)</span> is required to be measruable which is equivalent to say <span class="math inline">\(\phi\)</span> is measurable. It can be seen such integration concept comes from “partition the range” while the Riemann integration comes from partition of the domain. This is also shown in the construction of the approximation property. In particular, we define <span class="math inline">\(a\infty=0\)</span> when <span class="math inline">\(a=0\)</span> to deal with some special circumstances. For non-negative function we have two equivalent definition of integration.</p>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 2.6  </strong></span>Let <span class="math inline">\(f\)</span> be a non-negative Borel function and define its integral to be
<span class="math display">\[\int f d\nu=\underset{\phi\in S_f}{\mbox{sup}} \int \phi d\nu,\]</span> where <span class="math inline">\(S_f\)</span> is the collection of all non-negative simple function satisfying <span class="math inline">\(\phi(\omega) \leq f(\omega)\)</span> for any <span class="math inline">\(\omega \in \Omega\)</span>.</p>
</div>
<p>Another definition may be more suitable for operation, which comes from the well known Monotone Convergence Theorem.</p>
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>Definition 2.7  </strong></span>Let <span class="math inline">\(f\)</span> be a non-negative Borel function and define its integral to be
<span class="math display">\[\int f d\nu=\lim_{n \to \infty} \int f_n d\nu,\]</span> where <span class="math inline">\(0 \leq f_n \uparrow f\)</span> for <span class="math inline">\(f_n\)</span> is simple function for all <span class="math inline">\(n\)</span>.</p>
</div>
<p>For general function <span class="math inline">\(f\)</span>, its integral is defined as
<span class="math display">\[\int f d\nu=\int f_{+} d\nu-\int f_{-} d\nu,\]</span> we say this integral exists if and only if both integral on the right hand side are finite. Furthermore, we say <span class="math inline">\(f\)</span> is integrable if both integral are finite. Clearly, we have <span class="math inline">\(f\)</span> is integrable if and only if <span class="math inline">\(|f|\)</span> is since <span class="math inline">\(|f|=f_{+}+f_{-}\)</span>.</p>
<p>Below are some basic proposition:</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-14" class="proposition"><strong>Proposition 2.3  </strong></span>Let <span class="math inline">\(f\)</span> ang <span class="math inline">\(g\)</span> are Borel function. Then</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(f \leq g\)</span> and <span class="math inline">\(a \in \mathbb{R}\)</span>, then <span class="math inline">\(\int (af)\, d\nu\)</span> exists and is equal to <span class="math inline">\(a\int f \, d\nu\)</span>.</li>
<li>If both <span class="math inline">\(\int f \, d\nu\)</span> and <span class="math inline">\(\int g \, d\nu\)</span> exist and <span class="math inline">\(\int f \, d\nu+\int g \, d\nu\)</span> is well defined (not <span class="math inline">\(\infty-\infty\)</span>), then <span class="math inline">\(\int (f+g) \, d\nu\)</span> exists and is eual to <span class="math inline">\(\int f \, d\nu+\int g \, d\nu\)</span>.</li>
<li>If <span class="math inline">\(f \leq g\)</span> a.e., then <span class="math inline">\(\int f \, d\nu \leq \int g \, d\nu\)</span> if the integrals exist.</li>
<li>If <span class="math inline">\(f \geq 0\)</span> a.e. and <span class="math inline">\(\int f d\nu =0\)</span>, then <span class="math inline">\(f=0\)</span> a.e.</li>
<li><span class="math inline">\(\nu(A)=0\)</span> implies that <span class="math inline">\(\int_A f d\nu =0\)</span> where <span class="math inline">\(\int_A f d\nu := \int fI_A d\nu\)</span>.</li>
</ol>
</div>
<p>Here we also recall some classic theorem about limit and integral without proof in the next proposition.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-15" class="proposition"><strong>Proposition 2.4  </strong></span>Let <span class="math inline">\(f_1, f_2,\cdots,\)</span> be a sequence of Borel functions on <span class="math inline">\((\Omega,\cal F,\nu)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>(Monotone convergence theorem). If <span class="math inline">\(0 \leq f_1 \leq f_2 \leq \cdots\)</span> and <span class="math inline">\(\lim_\limits{n \to \infty} f_n=f\)</span> a.e., then <span class="math inline">\(\int \lim_\limits{n \to \infty} f_n d\nu=\lim_\limits{n \to \infty} \int f_n d\nu\)</span>.</li>
<li>(Dominated convergence theorem). If <span class="math inline">\(\lim_\limits{n \to \infty} f_n=f\)</span> a.e. and there exists an integrable function <span class="math inline">\(g\)</span> such that <span class="math inline">\(|f_n| \leq g\)</span> a.e., then <span class="math inline">\(\int \lim_\limits{n \to \infty} f_n d\nu=\lim_\limits{n \to \infty} \int f_n d\nu\)</span>.</li>
<li>(Fatous’s lemma). If <span class="math inline">\(f_n \geq 0\)</span>, then <span class="math inline">\(\int \lim_\limits{n \to \infty} f_n d\nu=\lim_\limits{n \to \infty} \int f_n d\nu\)</span>.</li>
</ol>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 2.2  </strong></span>Here we consider the interchange of differentiation and integration. That is, for fixed <span class="math inline">\(\theta \in \mathbb{R}\)</span>, let <span class="math inline">\(f(\omega, \theta)\)</span> be a Borel function on <span class="math inline">\((\Omega,\cal F,\nu)\)</span>. Assume that <span class="math inline">\(\partial f(\omega, \theta)/\partial \theta\)</span> exists a.e. for <span class="math inline">\(\theta \in (a,b) \subset \mathbb{R}\)</span> and that <span class="math inline">\(|\partial f(\omega, \theta)/\partial \theta| \leq g(\omega)\)</span> a.e., where <span class="math inline">\(g\)</span> is an integrable function on <span class="math inline">\(\Omega\)</span>. Then for each <span class="math inline">\(\theta \in (a,b)\)</span>, <span class="math inline">\(\partial f(\omega, \theta)/\partial \theta\)</span> is integrable and by mean value theorem and Dominated convergence theorem, we have <span class="math display">\[\frac{d}{d\theta} \int f(\omega, \theta) d\nu= \int (\partial f(\omega, \theta)/\partial \theta) \, d\nu.\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 2.3  </strong></span>Consider the moment generating function of a random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(M(t)\)</span> on a finite interval <span class="math inline">\((a,b)\)</span>. By the above example and the fact that <span class="math inline">\(|x|e^{t_0dx} \leq c_{+} e^{(t_0+\delta) x}+c_{-} e^{(t_0-\delta) x}\)</span>, where <span class="math inline">\(c_{+}=\underset{x \geq 0}{\max} \frac{|x|e^{tx}}{e^{(t_0+\delta)x} }\)</span> and <span class="math inline">\(c_{-}=\underset{x \leq 0}{\max} \frac{|x|e^{tx}}{e^{(t_0-\delta)x} }\)</span> for some <span class="math inline">\(t_0 \in (a,b)\)</span>, then we have <span class="math inline">\(M&#39;(t)=E(Xe^{tX})\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-18" class="theorem"><strong>Theorem 2.2  (Change of variables) </strong></span>Let <span class="math inline">\(f\)</span> be measurable from <span class="math inline">\((\Omega,\cal F,\nu)\)</span> to <span class="math inline">\((\Lambda,\cal G)\)</span> and <span class="math inline">\(g\)</span> be Borel on <span class="math inline">\((\Lambda,\cal G)\)</span>. Then
<span class="math display">\[\int_{\Omega} g(f(\omega)) d\nu(\omega)= \int_{\Lambda} g(x) d(\nu \circ f^{-1})(x),\]</span> where <span class="math inline">\(\nu \circ f^{-1}(B):= \nu(f^{-1}(B))\)</span> for <span class="math inline">\(B \in \cal G\)</span>.</p>
</div>
<p>Consider an easy case, let <span class="math inline">\(g=\sum_\limits{i=1}^k c_iI_{A_i}\)</span> for <span class="math inline">\(A_1,\cdots, A_k\)</span> disjoint. Then the right hand side is equal to <span class="math inline">\(\sum_\limits{i=1}^k c_i\, \nu\circ f^{-1}(A_i)\)</span> and let <span class="math inline">\(B_i=f^{-1}(A_i)\)</span>, then the equality holds clearly.</p>
<p>An important application of the theorem is that for random variable X with distribution <span class="math inline">\(P \circ X^{-1}\)</span>, we have
<span class="math display">\[ E(g(X))=\int g(X(\omega)) dP(\omega)=\int g(x) dP\circ X^{-1}(x).\]</span> Also, by the uniqueness theorem of measure, <span class="math inline">\(P\circ X^{-1}\)</span> coincides with the cumulative density function <span class="math inline">\(F(x):=P(X\leq x)\)</span>. We also denote <span class="math inline">\(P\circ X^{-1}\)</span> as <span class="math inline">\(P_X\)</span>, the distribution of <span class="math inline">\(X\)</span>. Below we consider the interchange of integration which is known as Fubini’s Theorem.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 2.3  (Fubini Theorem) </strong></span>Let <span class="math inline">\(\nu_i\)</span> be <span class="math inline">\(\sigma\)</span>-finite measure on <span class="math inline">\((\Omega_i,\cal F_i)\)</span> for <span class="math inline">\(i=1,2\)</span>, and let <span class="math inline">\(f\)</span> be Borel function on the product <span class="math inline">\(\sigma\)</span> algebra. Suppose <span class="math inline">\(f \geq 0\)</span> (w.r.t. Tonelli’s theorem) or <span class="math inline">\(f\)</span> integrable with respect to <span class="math inline">\(\nu_1 \times \nu_2\)</span>. Then
<span class="math display">\[\int_{\Omega_1} f(\omega_1,\omega_2) d\nu_1\]</span> exists <span class="math inline">\(\nu_2\)</span>-a.e. and is a Borel function on <span class="math inline">\(\Omega_2\)</span> whose integral with respect to <span class="math inline">\(\nu_2\)</span> exists, and
<span class="math display">\[\int_{\Omega_1 \times \Omega_2} f(\omega_1,\omega_2) d(\nu_1\times \nu_2)=\int_{\Omega_2}(\int_{\Omega_1} f(\omega_1,\omega_2) d\nu_1) d\nu_2.\]</span></p>
</div>
<p>The “<span class="math inline">\(\sigma\)</span>-finite” condition on the measures is assumed for the uniqueness of <span class="math inline">\(\nu_1 \times \nu_2\)</span>. It can be seen necessary if we consider the interchange of integral of an indicator function on which the set is an arbitrary set in the product field.</p>
<p>In the following We discuss the coincidence of Riemann integral and Lebesgue integral. An easy result can be shown on finite interval <span class="math inline">\((a,b)\)</span> that Riemann integrability implies Lebesgue integrability and the values of the integrals coincides. That is,
<span class="math display">\[\int_{(a,b)} f d\lambda=\int_a^b f(x) dx. \]</span> Furthermore, we can extend the result to the domain like <span class="math inline">\((0,\infty)\)</span> for a positive function <span class="math inline">\(f\)</span> by the monotone convergence theorem.</p>
<p>We end this chapter by the two classical examples. The first one is used for calculating the expectation of a positvie random variable.</p>
<div class="example">
<p><span id="exm:unlabeled-div-20" class="example"><strong>Example 2.4  </strong></span>Combining the result of Fubini’s Theorem and the above consequence, we can show that for <span class="math inline">\(X&gt; 0\)</span>, <span class="math display">\[E(X)=\int_0^{\infty}(1-F(t)) \, dt.\]</span> Let the <span class="math inline">\(\lambda\)</span> be Lebesgue measure, the RHS can be written as <span class="math inline">\(\int_{(0,\infty)} \int I_{(t, \infty)}(x) dP_X(x) \, d\lambda(t)\)</span>, then the result follows by applying Fubini’s Theorem.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-21" class="example"><strong>Example 2.5  </strong></span>For function <span class="math inline">\(f \geq 0\)</span> taken values on <span class="math inline">\(\Omega=(\omega_1,\omega_2,\cdots)\)</span>, by monotone convergence theorem and the fact that <span class="math inline">\(f=\lim_\limits{n\to \infty} \sum_\limits{i=1}^n f(\omega_i)\)</span> we have <span class="math inline">\(\int f d\nu= \sum_i f(\omega_i)\nu(\omega_i)\)</span>.</p>
</div>
</div>
<div id="radon-nikodym-derivative" class="section level3 hasAnchor" number="2.2.2">
<h3><span class="header-section-number">2.2.2</span> Radon-Nikodym Derivative<a href="integration-and-differentiation.html#radon-nikodym-derivative" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-22" class="definition"><strong>Definition 2.8  (absolutely continuous) </strong></span>Given two measures <span class="math inline">\(\mu,\nu\)</span> on <span class="math inline">\((\Omega, \cal F)\)</span>, we say <span class="math inline">\(\mu\)</span> is absolutely continuous with respect to <span class="math inline">\(\nu\)</span>, denoted by <span class="math inline">\(\mu \ll \nu\)</span>, if <span class="math inline">\(\nu(A)=0\)</span> implies <span class="math inline">\(\mu(A)=0\)</span> for any <span class="math inline">\(A \in \cal F\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 2.4  (Rando-Nikodym Theorem) </strong></span>Given two measures <span class="math inline">\(\mu,\nu\)</span> on <span class="math inline">\((\Omega, \cal F)\)</span> and <span class="math inline">\(\nu\)</span> is <span class="math inline">\(\sigma\)</span>-fintie. If <span class="math inline">\(\mu \ll\nu\)</span>, then there exists a nonnegative Borel function <span class="math inline">\(f\)</span>, which is unique <span class="math inline">\(\nu\)</span>-a.e. on <span class="math inline">\(\Omega\)</span> such that <span class="math display">\[\mu(A)=\int_A f d\nu.\]</span></p>
</div>
<p>Such function <span class="math inline">\(f\)</span> is called a Rando-Nikodym derivative or density and is denote by <span class="math inline">\(\frac{d\mu}{d\nu}\)</span>. A function <span class="math inline">\(f\geq 0\)</span> <span class="math inline">\(\nu\)</span>-a.e. is called probabiilty density function (p.d.f.) w.r.t a probabiity measure <span class="math inline">\(\mu\)</span> if <span class="math inline">\(\int f d\nu=1\)</span>. A discrete p.d.f. is a p.d.f w.r.t counting measure and a Lebesgue p.d.f corresponds to Lebesgue measure. A sufficient and necessary condition for a c.d.f. <span class="math inline">\(F\)</span> having a Lebesgue p.d.f is that <span class="math inline">\(F\)</span> is absolutely continuous. Below we consider a special case that both Lebesgue p.d.f and discrete p.d.f cannot be well defined.</p>
<div class="example">
<p><span id="exm:unlabeled-div-24" class="example"><strong>Example 2.6  </strong></span>Suppose that <span class="math inline">\(Z\)</span> is a standard normal r.v. and <span class="math inline">\(X=ZI_{[1,\infty]}(Z)\)</span>, clearly <span class="math inline">\(X\)</span> has no Lebesgue density since <span class="math inline">\(P_X({0})\neq 0\)</span>. Let <span class="math inline">\(\mu\)</span> be the probability measure on <span class="math inline">\((\mathbb{R},\cal B)\)</span> such that <span class="math inline">\(\delta_0(A)=I_A(0)\)</span>. First we claim that <span class="math inline">\(P_X \ll \delta_0+\lambda\)</span> since for any set <span class="math inline">\(A\in \cal F\)</span>, <span class="math inline">\((\delta_0+\lambda)(A)=0\)</span> implies <span class="math inline">\(0 \notin A\)</span> and <span class="math inline">\(\lambda(A)=0\)</span>. Then <span class="math inline">\(P_X(A)=P(Z\geq 1,Z \in A)+P(Z&lt;1,Z \in A)\stackrel{0\notin A}=P(Z\geq 1,Z \in A)\stackrel{\lambda(A)=0}=0,\)</span> which proves the claim. Secondly, we would like to find out the Radon-Nikodym derivative <span class="math inline">\(\frac{dP_X}{d(\delta_0+\lambda)}\)</span>. The density with respect to <span class="math inline">\((\delta_0+\lambda)\)</span> can be written as <span class="math inline">\(\Phi(0)I_{\{0\}}(x)+\frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}}I_{[1,\infty)}(x)\)</span>, where <span class="math inline">\(\Phi(x)\)</span> is the c.d.f of standard normal.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-25" class="remark"><em>Remark</em>. </span>Firstly, It can be noticed that <span class="math display">\[\int_A f\, d\delta_0=\int f(0)I_{A}(x)\, d\delta_0(x)=f(0)I_A(0).\]</span> The first equality holds since <span class="math inline">\(fI_{A}(x)=f(0)I_{A}(x)\)</span> <span class="math inline">\(\delta_0\)</span>-a.e.</p>
<p>Secondly, for the “overlapping case” such as consider the Randon-Nykodym derivative of <span class="math inline">\(ZI_{[1,\infty)}(z)+2I_{(0,1)}(z)\)</span>, the density is <span class="math inline">\(\phi(x)I_{(1,\infty)\setminus \{2\}}+P(X=2)I_2(x)+P(X=0)I_0(x)\)</span>. It is important that the set w.r.t first component has to consider minusing <span class="math inline">\(\{2\}\)</span>.</p>
</div>
<p>Below we list some propositions regarding to Randon-Nykodym derivative.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-26" class="proposition"><strong>Proposition 2.5  </strong></span>Let <span class="math inline">\(\nu\)</span> be a <span class="math inline">\(\sigma\)</span>-finite measure on a measure space <span class="math inline">\((\Omega,\cal F)\)</span>. Then</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\lambda\)</span> is a measure, <span class="math inline">\(\lambda \ll \nu\)</span>, and <span class="math inline">\(f\geq 0\)</span>, then <span class="math display">\[\int f d\lambda=\int f\frac{d\lambda}{d\nu} d\nu\]</span></li>
<li>If <span class="math inline">\(\lambda_i\ll \nu\)</span> for <span class="math inline">\(i=1,2\)</span>, then <span class="math inline">\(\lambda_1+\lambda_2\ll \nu\)</span> and
<span class="math display">\[\frac{d(\lambda_1+\lambda_2)}{d\nu}=\frac{d\lambda_1}{d\nu}+\frac{d\lambda_2}{d\nu} \quad \nu\mbox{-a.e.}\]</span></li>
<li>If <span class="math inline">\(\tau\)</span> is a measure, <span class="math inline">\(\lambda\)</span> is a <span class="math inline">\(\sigma\)</span>-finite measure, and <span class="math inline">\(\tau\ll\lambda\ll \nu\)</span>, then <span class="math display">\[\frac{d\tau}{d\nu}=\frac{d\tau}{d\lambda}\frac{d\lambda}{d\nu}\quad \nu\mbox{-a.e.}\]</span> In particular, if <span class="math inline">\(\lambda\ll \nu\)</span> and <span class="math inline">\(\nu \ll \lambda\)</span> (equivalent), then <span class="math inline">\(\frac{d\lambda}{d\nu}=(\frac{d\nu}{d\lambda})^{-1}\)</span>.</li>
</ol>
</div>
<p>The first result can be quickly verified by utilizing the approximation property and Monotone Convergence Theorem. The third result (chain rule) can be directly obtained by the first one.</p>
<p>Below we consider the density of transformation of random variables. A general result is given in proposition 1.8 of the textbook.</p>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 2.7  </strong></span>Suppose that <span class="math inline">\(X\)</span> is a random variable with Lebesgue p.d.f. <span class="math inline">\(f_X\)</span> and <span class="math inline">\(f_X(x)=0\)</span> for <span class="math inline">\(x\leq 0\)</span>. Let <span class="math inline">\(Y=X^2\)</span> and <span class="math display">\[g(y)=(2\sqrt{y})^{-1}f_X(\sqrt{y})I_{(0,\infty)}(y).\]</span>
Then <span class="math inline">\(g\)</span> is a Lebesgue p.d.f. of <span class="math inline">\(Y\)</span>.</p>
</div>
<p>To verify the above result, i.e. we want <span class="math inline">\(P(Y\in A)=\int_A g(y) d\lambda(y)\)</span> for <span class="math inline">\(A \in \cal B(\mathbb{R})\)</span>. It suffices to consider the case of intervals like <span class="math inline">\((-\infty,b]\)</span> (a <span class="math inline">\(\pi\)</span>-system) and let <span class="math inline">\(\lambda^{+}(A):=\int_A I_{(0,\infty)}(y) d\lambda(y)\)</span> (<span class="math inline">\(\frac{d\lambda^{+}}{d\lambda}(y)=I_{(0,\infty)}(y)\)</span>) and <span class="math inline">\(h(y)=\sqrt{y}I_{(0,\infty)}(y)\)</span>. Then for <span class="math inline">\(b&gt;0\)</span>,
<span class="math display">\[\begin{split}
\int_{(-\infty,b]} g(y) d\lambda(y)&amp;=\int_{(-\infty,b]} (2\sqrt{y})^{-1}f_X(\sqrt{y})I_{(0,\infty)}(y) d\lambda(y) \\
&amp;= \int_{(-\infty,b]} (2\sqrt{y})^{-1}f_X(\sqrt{y}) \frac{d\lambda^{+}}{d\lambda}(y) d\lambda(y) \\
&amp;=\int I_{(0,b)}(y) (2\sqrt{y})^{-1}f_X(\sqrt{y}) d\lambda^{+}(y)\\
&amp;=\int I_{(0,\sqrt{b})}(z) (2z)^{-1}f_X(z) d(\lambda^{+}\circ h^{-1})(z).
\end{split}\]</span></p>
<p>Note that <span class="math display">\[\lambda^{+}\circ h^{-1}((-\infty,b))=\lambda^{+}((0,b^2))=b^2=\int_{(-\infty,b)}2xI_{(0,\infty)}(x) d\lambda(x).\]</span> Thus <span class="math display">\[\frac{d\lambda^{+}\circ h^{-1}}{d\lambda}(x)=2xI_{(0,\infty)}(x).\]</span> Therefore by applying the first one in proposition 2.5 again, we can derive that <span class="math display">\[\int_{(-\infty,b]} g(y) d\lambda(y)=\int_{(0,\sqrt{b})}f_X(z) d\lambda(z)=P_X((0,\sqrt{b}))=P(Y\in (0,b]).\]</span></p>
<p>For the case <span class="math inline">\(f_x \neq 0\)</span> on <span class="math inline">\((-\infty,0)\)</span>, the RN (Radon-Nykodim derivative) is <span class="math display">\[ f_Y(y)=(\frac{f_x(\sqrt y)}{(2\sqrt y)}+\frac{f_x(-\sqrt y)}{(2\sqrt y)})I_{(0,\infty)}(y).\]</span></p>
<p>In summary, the proof is mainly based on (i) the change of measure and (ii) the integral formula w.r.t change of variables.</p>
<div class="remark">
<p><span id="unlabeled-div-28" class="remark"><em>Remark</em>. </span>Can the result above be generalized to a general measure other than Lebesgue measure?</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-spaces-and-random-elements.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conditional-expectation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-prob.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
