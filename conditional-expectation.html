<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Conditional Expectation | Topics in Mathematical Statistics</title>
  <meta name="description" content="This is my note of topics in mathematical statistics (under writing)." />
  <meta name="generator" content="bookdown 0.31 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Conditional Expectation | Topics in Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is my note of topics in mathematical statistics (under writing)." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Conditional Expectation | Topics in Mathematical Statistics" />
  
  <meta name="twitter:description" content="This is my note of topics in mathematical statistics (under writing)." />
  

<meta name="author" content="Huei-Yuan Yao" />


<meta name="date" content="2023-01-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="integration-and-differentiation.html"/>
<link rel="next" href="asymptotical-theory.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Topics in Mathematical Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prob.html"><a href="prob.html"><i class="fa fa-check"></i><b>2</b> Measure-based Probability theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html"><i class="fa fa-check"></i><b>2.1</b> Probability Spaces and Random Elements</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html#recap-measure-theory"><i class="fa fa-check"></i><b>2.1.1</b> Recap: Measure theory</a></li>
<li class="chapter" data-level="2.1.2" data-path="probability-spaces-and-random-elements.html"><a href="probability-spaces-and-random-elements.html#measurable-functions"><i class="fa fa-check"></i><b>2.1.2</b> Measurable Functions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html"><i class="fa fa-check"></i><b>2.2</b> Integration and Differentiation</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html#lebesgue-integral"><i class="fa fa-check"></i><b>2.2.1</b> Lebesgue integral</a></li>
<li class="chapter" data-level="2.2.2" data-path="integration-and-differentiation.html"><a href="integration-and-differentiation.html#radon-nikodym-derivative"><i class="fa fa-check"></i><b>2.2.2</b> Radon-Nikodym Derivative</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html"><i class="fa fa-check"></i><b>2.3</b> Conditional Expectation</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-expectation-1"><i class="fa fa-check"></i><b>2.3.1</b> Conditional Expectation</a></li>
<li class="chapter" data-level="2.3.2" data-path="conditional-expectation.html"><a href="conditional-expectation.html#independence"><i class="fa fa-check"></i><b>2.3.2</b> Independence</a></li>
<li class="chapter" data-level="2.3.3" data-path="conditional-expectation.html"><a href="conditional-expectation.html#conditional-distribution"><i class="fa fa-check"></i><b>2.3.3</b> Conditional Distribution</a></li>
<li class="chapter" data-level="2.3.4" data-path="conditional-expectation.html"><a href="conditional-expectation.html#markov-chains-and-martingales"><i class="fa fa-check"></i><b>2.3.4</b> Markov chains and Martingales</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html"><i class="fa fa-check"></i><b>2.4</b> Asymptotical Theory</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#convergence-modes"><i class="fa fa-check"></i><b>2.4.1</b> Convergence modes</a></li>
<li class="chapter" data-level="2.4.2" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.4.2</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="2.4.3" data-path="asymptotical-theory.html"><a href="asymptotical-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.4.3</b> Central Limit Theorem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-decision-theory.html"><a href="statistical-decision-theory.html"><i class="fa fa-check"></i><b>3</b> Statistical Decision Theory</a>
<ul>
<li class="chapter" data-level="3.1" data-path="general-framework.html"><a href="general-framework.html"><i class="fa fa-check"></i><b>3.1</b> General Framework</a></li>
<li class="chapter" data-level="3.2" data-path="point-estimators.html"><a href="point-estimators.html"><i class="fa fa-check"></i><b>3.2</b> Point Estimators</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="point-estimators.html"><a href="point-estimators.html#umvue"><i class="fa fa-check"></i><b>3.2.1</b> UMVUE</a></li>
<li class="chapter" data-level="3.2.2" data-path="point-estimators.html"><a href="point-estimators.html#u-statistics"><i class="fa fa-check"></i><b>3.2.2</b> U-Statistics</a></li>
<li class="chapter" data-level="3.2.3" data-path="point-estimators.html"><a href="point-estimators.html#v-statistics"><i class="fa fa-check"></i><b>3.2.3</b> V-Statistics</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html"><i class="fa fa-check"></i><b>3.3</b> Hypothesis Tests</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#ump-tests"><i class="fa fa-check"></i><b>3.3.1</b> UMP Tests</a></li>
<li class="chapter" data-level="3.3.2" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#tests-in-parametric-models"><i class="fa fa-check"></i><b>3.3.2</b> Tests in Parametric Models</a></li>
<li class="chapter" data-level="3.3.3" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#tests-in-nonparametric-models"><i class="fa fa-check"></i><b>3.3.3</b> Tests in Nonparametric Models</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="confidence-sets.html"><a href="confidence-sets.html"><i class="fa fa-check"></i><b>3.4</b> Confidence Sets</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="estimation-in-parametric-models.html"><a href="estimation-in-parametric-models.html"><i class="fa fa-check"></i><b>4</b> Estimation in Parametric Models</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bayes-estimators.html"><a href="bayes-estimators.html"><i class="fa fa-check"></i><b>4.1</b> Bayes Estimators</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="bayes-estimators.html"><a href="bayes-estimators.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.1.1</b> Markov chain Monte Carlo</a></li>
<li class="chapter" data-level="4.1.2" data-path="bayes-estimators.html"><a href="bayes-estimators.html#asymptotic-efficiency-of-bayes-estimators"><i class="fa fa-check"></i><b>4.1.2</b> Asymptotic efficiency of Bayes Estimators</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html"><i class="fa fa-check"></i><b>4.2</b> Method of Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#mle"><i class="fa fa-check"></i><b>4.2.1</b> MLE</a></li>
<li class="chapter" data-level="4.2.2" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#quasi-likelihoods-and-conditional-likelihoods"><i class="fa fa-check"></i><b>4.2.2</b> Quasi-likelihoods and conditional likelihoods</a></li>
<li class="chapter" data-level="4.2.3" data-path="method-of-maximum-likelihood.html"><a href="method-of-maximum-likelihood.html#asymptotic-efficiency"><i class="fa fa-check"></i><b>4.2.3</b> Asymptotic efficiency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation-in-nonparametric-models.html"><a href="estimation-in-nonparametric-models.html"><i class="fa fa-check"></i><b>5</b> Estimation in Nonparametric Models</a>
<ul>
<li class="chapter" data-level="5.1" data-path="distributional-estimators.html"><a href="distributional-estimators.html"><i class="fa fa-check"></i><b>5.1</b> Distributional Estimators</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-functionals.html"><a href="statistical-functionals.html"><i class="fa fa-check"></i><b>5.2</b> Statistical Functionals</a></li>
<li class="chapter" data-level="5.3" data-path="sample-quantiles.html"><a href="sample-quantiles.html"><i class="fa fa-check"></i><b>5.3</b> Sample quantiles</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Estimating Equations</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#framework"><i class="fa fa-check"></i><b>5.4.1</b> Framework</a></li>
<li class="chapter" data-level="5.4.2" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#consistency"><i class="fa fa-check"></i><b>5.4.2</b> Consistency</a></li>
<li class="chapter" data-level="5.4.3" data-path="generalized-estimating-equations.html"><a href="generalized-estimating-equations.html#asymptotic-normality"><i class="fa fa-check"></i><b>5.4.3</b> Asymptotic normality</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="generalized-methods-of-moments.html"><a href="generalized-methods-of-moments.html"><i class="fa fa-check"></i><b>5.5</b> Generalized Methods of Moments</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>6</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Topics in Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conditional-expectation" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Conditional Expectation<a href="conditional-expectation.html#conditional-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="conditional-expectation-1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Conditional Expectation<a href="conditional-expectation.html#conditional-expectation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-29" class="definition"><strong>Definition 2.9  </strong></span>Let <span class="math inline">\(X\)</span> be a integrable random variable on <span class="math inline">\((\Omega, \cal F,P)\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(\cal A\)</span> be a sub <span class="math inline">\(\sigma\)</span>-field of <span class="math inline">\(\cal F\)</span>. The condition expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(\cal A\)</span> (which we denote it as <span class="math inline">\(E(X|\cal A)\)</span>) is the a.s.-unique random variable satisfying <span class="math inline">\(E(X|\cal A)\)</span> is measurable from <span class="math inline">\((\Omega,\cal A)\)</span> to <span class="math inline">\((R,\cal B)\)</span> and <span class="math inline">\(\int_A E(X|\cal A) \rm dP=\)</span> <span class="math inline">\(\int_{A} X dP\)</span> for all <span class="math inline">\(A \in \cal A\)</span>.</p></li>
<li><p>Let <span class="math inline">\(B \in \cal F\)</span>. The conditional probability of <span class="math inline">\(B\)</span> given <span class="math inline">\(\cal A\)</span> is defined to be <span class="math inline">\(P(B|\cal{A})=\)</span> <span class="math inline">\(E(I_B|\cal A)\)</span>.</p></li>
<li><p>Let <span class="math inline">\(Y\)</span> be measurable from <span class="math inline">\((\Omega, \cal F,P)\)</span> to <span class="math inline">\((\Lambda, \cal G)\)</span>. The conditional expectation of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is defined to be <span class="math inline">\(E(X|Y)=E(X|\sigma(Y)).\)</span></p></li>
</ol>
</div>
<p>Define <span class="math inline">\(\mu^{+}=\int_A X^{+} dP\)</span> for <span class="math inline">\(A \in \cal A\)</span>, then such <span class="math inline">\(\mu^{+}\)</span> is a measure on <span class="math inline">\(\cal A\)</span>. Let <span class="math inline">\(P_0\)</span> be the restriction of <span class="math inline">\(P\)</span> on <span class="math inline">\(\cal A\)</span>. Then clearly we have <span class="math inline">\(\mu^{+}\ll P_0\)</span>. It is easy to check that <span class="math inline">\(E(X^{+}|\cal A)= \frac{d\mu^{+}}{dP_0}\)</span> and <span class="math inline">\(E(X^{-}|\cal A)= \frac{d\mu^{-}}{dP_0}\)</span> (by similarly defining <span class="math inline">\(\mu^{-}\)</span>) will satisfy the definition of conditional expectation. The uniqueness and existence follows by RN theorem.</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 2.8  </strong></span>Suppose <span class="math inline">\(\Omega={1,2,3,4}\)</span> and <span class="math inline">\(P(\{k\})=\frac{1}{4}\)</span> for <span class="math inline">\(k\in\Omega\)</span>. Suppose that <span class="math inline">\(X(k)=k\)</span> for <span class="math inline">\(k\in\Omega\)</span>. Let <span class="math inline">\(Y(1)=4,\ Y(2)=5,\ Y(3)=Y(4)=6\)</span>. Find <span class="math inline">\(E(X|\sigma(Y))=h(Y)\)</span>.
Directly by calculating that <span class="math inline">\(\int_A E(X|\cal A) \rm dP=\)</span> <span class="math inline">\(\int_{A} X dP\)</span>, we can derive that <span class="math inline">\(h(4)=1,\ h(5)=2,\ h(6)=\frac{7}{2}\)</span>.</p>
</div>
<p>If we consider trivial <span class="math inline">\(\sigma\)</span>-algebra <span class="math inline">\(\cal A=\{\phi,\ \Omega\}\)</span>, then by measurability we know that <span class="math inline">\(E(X|\cal A)\)</span> must be a constant function. By the integral restriction, clearly <span class="math inline">\(E(X|\cal A)=\rm E(X)\)</span>. Furthermore, suppose <span class="math inline">\(X\)</span> is measurable w.r.t <span class="math inline">\(\cal A_0\)</span> which is a sub <span class="math inline">\(\sigma\)</span>-field of <span class="math inline">\(\cal A\)</span>. Then <span class="math inline">\(E(X|\cal A)=\rm X\)</span>.</p>
<p>Note that <span class="math inline">\(E(X|\sigma(Y))=E(X|Y)=h(Y)\)</span> for some <span class="math inline">\(h\)</span> by lemma 2.1. Thus we may write <span class="math inline">\(E(X|Y=y)=h(y)\)</span>. Below we give some proposition regarding to conditoinal expectation.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-31" class="proposition"><strong>Proposition 2.6  </strong></span>Given the same setup above, we have</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(X=c\)</span> a.s. for <span class="math inline">\(c \in \mathbb{R}\)</span> then <span class="math inline">\(E(X|\cal A)=c\)</span> a.s. (measurability is trivial for a constant function).</li>
<li>If <span class="math inline">\(X\leq Y\)</span> a.s., then <span class="math inline">\(E(X|\cal A)\leq \rm E(Y|\cal A)\)</span> a.s. (which can be quickly proved by linearity)</li>
<li>For <span class="math inline">\(E|X|,\ E|Y|&lt; \infty\)</span>, <span class="math inline">\(E(aX+bY|\cal A)=a\rm E(X|\cal A)+b\rm E(Y|\cal A)\)</span>.</li>
<li><span class="math inline">\(E(E(X|\cal A))= \int_{\Omega} \rm E(X|\cal A) \rm dP=\int_{\Omega} X dP=E(X)\)</span>.</li>
<li>Let <span class="math inline">\(\cal A_0 \subset \cal A\)</span> be sub <span class="math inline">\(\sigma\)</span>-fields of some <span class="math inline">\(\cal F\)</span>. Then <span class="math inline">\(E(E(X|\cal A)|\cal A_0))=\rm E(\rm E(X|\cal A_0)|\cal A))=\rm E(X|\cal A_0)\)</span>.</li>
<li>If <span class="math inline">\(\sigma(Y)\subset \cal A\)</span> and <span class="math inline">\(E(|XY|)&lt; \infty\)</span>, then <span class="math inline">\(E(XY|\cal A)=\rm YE(X|\cal A)\)</span>.</li>
<li>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, <span class="math inline">\(g\)</span> is Borel function and <span class="math inline">\(E|g(X,Y)|&lt;\infty\)</span>. Let <span class="math inline">\(h(y)=E(g(X,y))\)</span> for all <span class="math inline">\(y \in Y\)</span>. Then <span class="math inline">\(E(g(X,Y)|Y)=h(Y)\)</span>, or equivalently, <span class="math inline">\(E(g(X,Y)|Y=y)=h(y)\)</span>.</li>
<li>If <span class="math inline">\(E(X^2)&lt; \infty\)</span>, then <span class="math inline">\([E(X|\cal A)]^2\leq \rm E(X^2|\cal A)\)</span> a.s.</li>
</ol>
</div>
<div class="remark">
<p><span id="unlabeled-div-32" class="remark"><em>Remark</em>. </span>We briefly discuss how to show some of the properties as following:</p>
<ol style="list-style-type: decimal">
<li>For 6. we start from considering <span class="math inline">\(Y\)</span> is a simple function and use LDCT on general measurable <span class="math inline">\(Y\)</span>.</li>
<li>For 7. let <span class="math inline">\(g(X,Y)=I_A(X)I_B(Y)\)</span>, then <span class="math inline">\(\int_C h(y)dP_Y(y)=P(X\in A)P(Y\in B\cap C)\)</span>. On the other hand, <span class="math inline">\(\int_C I_A(X)I_B(Y) dP_Y(y)=P(X\in A, Y\in B\cap C)\)</span>, so the result follows by independence.</li>
<li>For 8., we directly show that by <span class="math inline">\(0 \leq E[X-E(X|\cal A)^2|\cal A]=\rm E(X^2|\cal A)- \rm (E(X|\cal A))^2\)</span>. The equality follows by linearity and 6.</li>
</ol>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-33" class="example"><strong>Example 2.9  </strong></span>This example shows that <span class="math inline">\(E(X|\cal A)\)</span> is the best guess of <span class="math inline">\(X\)</span> given some knowledge of <span class="math inline">\(\cal A\)</span>, which means <span class="math display">\[\int (X-E(X|\cal A))^2 \rm dP\leq \int (X-Y)^2 \rm dP\]</span> for any <span class="math inline">\(Y\)</span> measurable w.r.t. <span class="math inline">\(\cal A\)</span>. Let <span class="math inline">\(Z=Y-E(X|\cal A)\)</span> measurable w.r.t. <span class="math inline">\(\cal A\)</span>. It follows by that <span class="math inline">\(\int Z(X-E(X|\cal A)) \rm dP=E(E(Z(X-E(X|\cal A))|\cal A))=0\)</span>.</p>
</div>
</div>
<div id="independence" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Independence<a href="conditional-expectation.html#independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we extend the definition of independence to <span class="math inline">\(\sigma\)</span>-algebra.</p>
<div class="defintion">
<p>Let <span class="math inline">\((\Omega,\cal F,P)\)</span> be a probability space.</p>
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(\cal C\)</span> be a collection of subsets in <span class="math inline">\(\cal F\)</span>. Events in <span class="math inline">\(\cal C\)</span> is said to be independent if for any <span class="math inline">\(n \in \mathbb{N}\)</span> and distinct events <span class="math inline">\(A_1,\cdots,A_n\)</span> in <span class="math inline">\(\cal C\)</span>, we have <span class="math inline">\(P(A_1\cap\cdots A_n)=\prod_{i=1}^n P(A_i)\)</span>.</p></li>
<li><p>Collections <span class="math inline">\(\cal C_i\subset \cal F,\, i\in I\)</span> are said to be independent if events in any collection of the form <span class="math inline">\(\{A_i\in\cal C_i:i\in I\}\)</span> are independent.</p></li>
<li><p>Random elements <span class="math inline">\(X_i\)</span> are independent if <span class="math inline">\(\sigma(X_i)\)</span> are independent.</p></li>
</ol>
</div>
<p>Suppose that <span class="math inline">\(X\)</span> is a random variable on <span class="math inline">\((\Omega,\cal F,P)\)</span> with finite moment and <span class="math inline">\(\cal A_1\)</span> and <span class="math inline">\(\cal A_2\)</span> are sub <span class="math inline">\(\sigma\)</span>-fields of <span class="math inline">\(\cal F\)</span>. If <span class="math inline">\(\sigma(\sigma(X)\cup\cal A_1)\)</span> and <span class="math inline">\(\cal A_2\)</span> are independent, then <span class="math display">\[E(X|\sigma(\cal A_1\cup \cal A_2))=\rm E(X|\cal A_1)\quad a.s.\]</span> In fact, it is sufficient to show that
<span class="math display">\[\int_{\cal A_1\cap \cal A_1}E(X|\cal A_1)\rm dP= \int_{\cal A_1\cap \cal A_1}X dP.\]</span>
for any <span class="math inline">\(A_1\in \cal A_1\)</span> and <span class="math inline">\(A_2\in \cal A_2\)</span> since <span class="math inline">\(\mathbb{C}=\rm \{A_1\cap A_2| A_1\in \cal A_1,A_2\in \cal A_2\}\)</span> is a <span class="math inline">\(\pi\)</span>-system and <span class="math inline">\(\sigma(\mathbb{C})=\sigma(\cal A_1\cup \cal A_2)\)</span>. This result can be further established by the fact that <span class="math inline">\(E(E(X|A_1) I_{A_2})=\rm E(X|A_1)P(A_2)\)</span> given the assumption.</p>
<p>As a special case, <span class="math inline">\(E(X|Y_1,Y_2)=E(X|Y_1)\)</span> if <span class="math inline">\((X,Y_1)\)</span> and <span class="math inline">\(Y_2\)</span> are independent by the result of the exercise below (by replacing <span class="math inline">\(\cal A_1\)</span>, <span class="math inline">\(\cal A_2\)</span> with <span class="math inline">\(\sigma(Y_1)\)</span> and <span class="math inline">\(\sigma(Y_2)\)</span>). It still holds if replacing the random variable <span class="math inline">\(X\)</span> with <span class="math inline">\(h(X)\)</span> for any Borel function <span class="math inline">\(h\)</span>. In particular, with taking <span class="math inline">\(h\)</span> as indicator function, we have <span class="math display">\[P(A|Y_1,Y_2)=P(A|Y_1)\]</span> for any <span class="math inline">\(A\in \sigma(X)\)</span> if <span class="math inline">\((X,Y_1)\)</span> and <span class="math inline">\(Y_2\)</span> are independent. In such case, we say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_2\)</span> are <strong>conditionally independent</strong> given non-constant <span class="math inline">\(Y_1\)</span>.</p>
<p>Also, if <span class="math inline">\(E|X|&lt;\infty\)</span>, <span class="math inline">\(\sigma(X)\)</span> and <span class="math inline">\(\sigma(Y)\)</span> are independent, then <span class="math inline">\(E(X|Y)=E(X)\)</span> a.s.</p>
<div class="exercise">
<p><span id="exr:unlabeled-div-34" class="exercise"><strong>Exercise 2.1  </strong></span>Let <span class="math inline">\(Z=(Y_1,Y_2)\)</span>, <span class="math inline">\(\sigma(Z)\stackrel{?}=\sigma(\sigma(Y_1)\cup\sigma(Y_2))\)</span>.</p>
<p>First we show that <span class="math display">\[\sigma(\sigma(Y_1)\cup\sigma(Y_2))=\sigma(\{Y_1^{-1}(B_1)\cap\sigma(Y_2^{-1}(B_2)):B_1\in \cal B^n, \rm B_2\in \cal B^m\}).\]</span> Then the result follows if <span class="math display">\[\sigma(Z)=\sigma(\{Y_1^{-1}(B_1)\cap\sigma(Y_2^{-1}(B_2)):B_1\in \cal B^n, \rm B_2\in \cal B^m\}).\]</span></p>
<p>For the first equality, notice that <span class="math inline">\(\subseteq\)</span>-direction is clear since both <span class="math inline">\(\sigma(Y_1)\)</span> and <span class="math inline">\(\sigma(Y_2)\)</span> lie in the <span class="math inline">\(\sigma\)</span>-field on the right hand side. For another direction, note that for any <span class="math inline">\(B_1\)</span> and <span class="math inline">\(B_2\)</span>, <span class="math inline">\(Y_i^{-1}(B_i) \in \sigma(Y_1)\cup \sigma(Y_2),\, i=1,2\)</span>, thus the intersection must lie in the <span class="math inline">\(\sigma\)</span>-field on the left hand side.</p>
<p>For second equality, the <span class="math inline">\(\supseteq\)</span>-direction is obvious since the set <span class="math inline">\(\mathbb{D}:=\{Y_1^{-1}(B_1)\cap\sigma(Y_2^{-1}(B_2)):B_1\in \cal B^n, \rm B_2\in \cal B^m\}\)</span> is just <span class="math inline">\(\{Z^{-1}(B_1\times B_2):B_1\in \cal B^n, \rm B_2\in \cal B^m\}\)</span>. For another direction, note that for <span class="math inline">\(\cal B^{n+m}=\sigma(\cal B^n\times \cal B^m)\)</span> and the fact that <span class="math inline">\(Z^{-1}(\sigma(\mathbb{D})) \subseteq \sigma(Z^{-1}(\mathbb{D}))\)</span> (in fact, they are equal). The fact can be shown by proving the set <span class="math inline">\(\cal E:=\{\rm A| Z^{-1}(A)\in \sigma(Z^{-1}(\mathbb{D}))\}\)</span> is a <span class="math inline">\(\sigma\)</span>-field including the collection <span class="math inline">\(\mathbb{D}\)</span>. Then the result follows from <span class="math inline">\(\sigma(\mathbb{D})\subseteq \cal E\)</span>.</p>
</div>
</div>
<div id="conditional-distribution" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Conditional Distribution<a href="conditional-expectation.html#conditional-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First we define <span class="math inline">\(\mu(B,Y)=E(I_B(X)|Y)\)</span>. In other words, <span class="math inline">\(\int_{Y^{-1}(C)} \mu(B,Y)dP=\int_{Y^{-1}(C)} I_B(X)dP\)</span>. Furthermore, If <span class="math display">\[\int I_C(y)[\int_B f_{X|Y=y}(x) d\mu(x)]dP_Y(y)=P((X,Y)\in B\times C)\]</span> for <span class="math inline">\(B\in \cal B_X\)</span> and <span class="math inline">\(C \in \cal B_\rm{Y}\)</span>, then we say <span class="math inline">\(f_{X|Y=y}(x)\)</span> is the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> w.r.t <span class="math inline">\(\mu\)</span>. In fact, such function <span class="math inline">\(\mu(B,Y)\)</span> is called a <strong>random probability measure</strong>.</p>
<p>Suppose <span class="math inline">\((X,Y)\)</span> has a joint density function <span class="math inline">\(f_{X,Y}\)</span> w.r.t <span class="math inline">\(\mu \times \nu\)</span>. First of all, we show the result (<span class="math inline">\(f_{X|Y=y}(x)\)</span> is the conditional pdf w.r.t. <span class="math inline">\(\mu\)</span>) in basic probability that the conditional can be written as joint over marginal. Let <span class="math inline">\(f_{X|Y=y}(x)=\frac{f_{X,Y}(x,y)}{f_Y(y)}\)</span> for <span class="math inline">\(f_Y\)</span> is marginal pdf w.r.t. <span class="math inline">\(\nu\)</span>. The result can be validated by <span class="math display">\[\int I_C(y)(\int_B \frac{f_{X,Y}(x,y)}{f_Y(y)}\, d\mu(x))dP_Y(y)=\int_{B\times C }f_{X,Y}(x,y) d(\mu\times \nu)(x,y).\]</span> On the contrary, let <span class="math inline">\(g(x,y)=f_{X|Y=y}(x)f_Y(y),\)</span> then <span class="math inline">\(g\)</span> is the pdf of <span class="math inline">\((X,Y)\)</span> w.r.t. <span class="math inline">\(\mu\times \nu\)</span>. In other words, we would like to show that <span class="math display">\[\int_{B\times C} g(x,y)\, d(\mu\times \nu)(x,y)=P((X,Y)\in B\times C)\]</span> for <span class="math inline">\(B\in \cal B^m\)</span> and <span class="math inline">\(C\in \cal B^n\)</span>. Indeed, the result directly follows the definition of conditional pdf and density of <span class="math inline">\(Y\)</span>.</p>
<p>Secondly, we would like to show if <span class="math inline">\(E(X|Y=y)=\int xf_{X|Y=y}(x) d\mu(x)\)</span>. In other words, we may validate that <span class="math display">\[E[XI_C(y)]=\int I_C(y) [\int xf_{X|Y=y}(x) d\mu(x)] dP_Y(y),\]</span> by the definition, which can be quickly proved by approximating <span class="math inline">\(X\)</span> with simple functions.</p>
<div class="example">
<p><span id="exm:unlabeled-div-35" class="example"><strong>Example 2.10  </strong></span>Consider a simple case for <strong>Bayesian variable selection</strong> as the following, <span class="math inline">\((X_{i1},X_{i2},Y_i):=\cal D\)</span> which is i.i.d. random sample given <span class="math inline">\((a_1,a_2)\)</span> and follows the model <span class="math inline">\(Y=a_1X_1+a_2X_2+\epsilon\)</span>, where <span class="math inline">\((X_1,X_2,\epsilon)\)</span> are assumed to be mutually independent. <span class="math inline">\(X_1,X_2\)</span> has density <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> w.r.t <span class="math inline">\(\lambda\)</span> given <span class="math inline">\((a_1,a_2)\)</span>, also assume <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span> for some <span class="math inline">\(\sigma&gt;0\)</span>. In particular, we put prior <span class="math inline">\(\pi_1\times \pi_1\)</span> on <span class="math inline">\((a_1,a_2)\)</span> with <span class="math inline">\(\pi_1\)</span>’s density being <span class="math display">\[f_0:=\frac{d\pi_1(a)}{d(\lambda+\mu_0)}=c_0I_{\{0\}}(a)+(1-c_0)\phi(a)I_{\{0\}^c}(a)\]</span> w.r.t. measure <span class="math inline">\((\lambda+\mu_0)\)</span>, where <span class="math inline">\(\phi(a)\)</span> denotes the density of standard normal, <span class="math inline">\(\lambda\)</span> is Lebesgue measure, <span class="math inline">\(\mu_0\)</span> is point mass measure on <span class="math inline">\(0\)</span> and <span class="math inline">\(c_0\in(0,1)\)</span>. Calculate the corresponding posterior.</p>
<p>Let <span class="math inline">\(\phi_{\sigma}\)</span> be the pdf of <span class="math inline">\(N(0,\sigma^2)\)</span>. Clearly the joint density of <span class="math inline">\((\cal D,a_1,a_2)\)</span> is <span class="math display">\[\prod_{i=1}^n\, \phi_{\sigma}(y_i-(a_1x_{1,i}+a_2x_{2,i}))f_1(x_{1,i})f_2(x_{2,i})f_0(a_1)f_0(a_2):=h(a_1,a_2)c(\tilde{x})\]</span> w.r.t <span class="math inline">\(\lambda^3\times (\lambda+\mu_0)^2\)</span>. In addition, the marginal density of <span class="math inline">\(\cal D\)</span> can be calculated by <span class="math display">\[c(\tilde{x})(\int\int h(a_1,a_2) d\mu_0(a_1)d\mu_0(a_2)+ \int\int h(a_1,a_2) d\lambda(a_1)d\lambda(a_2)).\]</span>
We omit the detailed calculation of marginal here. (In particular, note that <span class="math inline">\(\int g(a) d\mu_0(a)=g(0)\)</span>.) Denote the posterior density as <span class="math inline">\(f_3\)</span>. Remarkably, the posterior probability of <span class="math inline">\((a_1,a_2)=(0,0)\)</span> is <span class="math display">\[\tilde{\pi}((a_1,a_2)=(0,0))=\int_{(0,0)} f_3(a_1,a_2)d((\lambda+\mu_0)\times(\lambda+\mu_0))(a_1,a_2)=f_3(0,0).\]</span> Similarly, the posterior probability of <span class="math inline">\(a_1=0\)</span> (or <span class="math inline">\(a_2=0\)</span>) can be also derived.</p>
</div>
<p>Finally, we consider a more general case when joint pdf may not exist with respect to product measure in the above scenario (<span class="math inline">\((X,Y)\)</span>). An application for such consideration is factorization theorem for finding sufficient statistics. For instance, suppose that <span class="math inline">\((X,Y)\)</span>’s distribution has pdf <span class="math inline">\(f_{X,Y}\)</span> w.r.t. <span class="math inline">\(P_{X_0,Y_0}\)</span> for some <span class="math inline">\((X_0,Y_0)\)</span> (dim(<span class="math inline">\(X\)</span>)=dim(<span class="math inline">\(X_0\)</span>) and dim(<span class="math inline">\(Y\)</span>)=dim(<span class="math inline">\(Y_0\)</span>)). Let <span class="math display">\[ f_Y(y)=\int f_{X,Y}(x,y) dP_{X_0|Y_0=y}(x),\]</span> then <span class="math inline">\(f_Y\)</span> is a pdf of <span class="math inline">\(Y\)</span> w.r.t. <span class="math inline">\(P_{Y_0}\)</span>. In other words, we can verify that <span class="math display">\[P(Y\in B)=\int I_B(y)\int f_{X,Y}(x,y) dP_{X_0|Y_0=y}(x) dP_{Y_0}(y).\]</span> Indeed, (from the exercise below) the right hand side of the equality can be written as <span class="math display">\[E[I_B(Y_0)f_{X,Y}(X_0,Y_0)]=\int I_B(y)f_{X,Y}(x,y) dP_{X_0,Y_0}(x,y)=P(Y\in B),\]</span> since <span class="math inline">\(f_{X,Y}\)</span> is the pdf w.r.t. <span class="math inline">\(P_{X_0,Y_0}\)</span>.</p>
<p>Next, we can check that <span class="math inline">\(f_{X|Y=y}(y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}\)</span> is the conditional pdf of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> w.r.t. <span class="math inline">\(P_{X_0|Y_0=y}\)</span>. That is to say, we shall validate (which is clear) <span class="math display">\[\int_C \int _B \frac{f_{X,Y}(x,y)}{f_Y(y)} dP_{X_0|Y_0=y}(x) f_Y(y) dP_{Y_0}(y)= P((X,Y)\in B\times C).\]</span></p>
<div class="exercise">
<p><span id="exr:unlabeled-div-36" class="exercise"><strong>Exercise 2.2  </strong></span>Let <span class="math inline">\(g(x,y)=\sum_i a_i I_{B_i}(x)I_{C_i}(y)\)</span> (an approximation to a Borel function), prove that <span class="math inline">\(\int g(x,y) dP_{X_0|Y_0=y}(x)=E(g(X_0,Y_0)|Y_0=y)\)</span>.</p>
<p>Since <span class="math display">\[ \int\sum_i a_iI_{C_i}(y) I_{B_i}(x) dP_{X_0|Y_0=y}(x)= E(\sum_i a_iI_{C_i}(Y_0)I_{B_i}(X_0)|Y_0=y),\]</span> which is directly <span class="math inline">\(E(g(X_0,Y_0)|Y_0=y)\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-37" class="example"><strong>Example 2.11  </strong></span>Let <span class="math inline">\(X:=(X_1,\cdots,X_n)\)</span> i.i.d from <span class="math inline">\(N(\mu,1)\)</span> and <span class="math inline">\(Y:=(Y_1,\cdots,Y_n)\)</span> i.i.d. from <span class="math inline">\(N(0,1)\)</span>. It can be seen that <span class="math inline">\((X,\bar{X})\)</span>’s distribution does not have density w.r.t. product Lebesgue measure. Instead, we can consider the density of <span class="math inline">\((X,\bar{X})\)</span> w.r.t. <span class="math inline">\(P(Y,\bar{Y})\)</span>. First we may show that <span class="math inline">\(P_{X,\bar{X}}\ll P_{Y,\bar{Y}}\)</span>. If we define a transformation <span class="math display">\[T(y_1,\cdots,y_n)=((y_1,\cdots,y_n),\frac{\sum_i y_i}{n}).\]</span> Then <span class="math display">\[P_{Y,\bar{Y}}(A)=P(Y\in T^{-1}(A))=P_Y(T^{-1}(A))=0,\]</span> implies <span class="math inline">\(\lambda^n(T^{-1}(A))=0\)</span>. Similarly we can argue <span class="math display">\[P_{X,\bar{X}}(A)=P_X(T^{-1}(A))=0.\]</span> Thus <span class="math inline">\(P_{X,\bar{X}}\ll P_{Y,\bar{Y}}\)</span>.</p>
<p>Secondly, we claim that <span class="math inline">\(\frac{dP_X}{dP_Y}\)</span> is the density of <span class="math inline">\((X,\bar{X})\)</span> w.r.t. <span class="math inline">\(P_{Y,\bar{Y}}\)</span>, where <span class="math inline">\(\frac{dP_X}{dP_Y}:=\frac{\frac{dP_X}{d\lambda^n}}{\frac{dP_Y}{d\lambda^n}}\)</span>. That is to say, we need to verify that <span class="math display">\[\int_{A\times B}\frac{dP_X}{dP_Y}(x)dP_{Y,\bar{Y}}(x,s)=P_{X,\bar{X}}(A\times B).\]</span> The LHS can be written as <span class="math inline">\(E[I_A(Y)I_B(\bar{Y})\frac{dP_X}{dP_Y}(Y)]\)</span>, which turns out to be <span class="math display">\[E[I_{T^{-1}(A\times B)}(Y)\frac{dP_X}{dP_Y}(Y)].\]</span> On the other hand, the RHS is just <span class="math inline">\(P_X(T^{-1}(A\times B))\)</span>, which is <span class="math display">\[P_X(T^{-1}(A\times B))=\int I_{T^{-1}(A\times B)}\frac{dP_X}{dP_Y}(y)dP_Y(y).\]</span> Therefore, the LHS agrees on RHS.</p>
<p>Lastly, we would like to write down the conditional probability. Since the joint density is given above. It remains to calculate the marginal density, that is, <span class="math display">\[f_{\bar{X}}(s)=\int f_{X,\bar{X}}(x,s) dP_{Y|\bar{Y}=s}(x)\]</span> w.r.t. <span class="math inline">\(P_{\bar{Y}}\)</span>. The key observation here is the joint density <span class="math inline">\(\frac{dP_X}{dP_Y}\)</span> can be written as
<span class="math display">\[\frac{dP_X}{dP_Y}=e^{-\frac{(n(\bar{X}-\mu)^2+n\bar{X}^2)}{2}},\]</span> which is a function only dependent on <span class="math inline">\(\bar{X}\)</span> and parameter <span class="math inline">\(\mu\)</span>. Furthermore, the marginal density of <span class="math inline">\(\bar{X}\)</span> w.r.t. <span class="math inline">\(P_{Y|\bar{Y}=s}\)</span> is <span class="math display">\[\int \frac{dP_X}{dP_Y}(x) dP_{Y|\bar{Y}=s}(x)=e^{-\frac{(n(s-\mu)^2+ns^2)}{2}}.\]</span> Therefore the conditional density is <span class="math inline">\(1\)</span>. In particular, recall the definition of sufficient statistics. The result shows that <span class="math inline">\(\bar{X}\)</span> is indeed the sufficient statistic of <span class="math inline">\(\mu\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-38" class="example"><strong>Example 2.12  </strong></span>Below we see a generalization of the result in the last example. See lemma 2.1 in the textbook. Let <span class="math inline">\(X=(X_1,\cdots,X_n)\)</span> with <span class="math inline">\(P_X\in\{P_{\theta}:\theta \in \Theta\}\)</span> has dominating <span class="math inline">\(\sigma\)</span>-finite measure <span class="math inline">\(\nu\)</span>. Denote the pdf of <span class="math inline">\(X\)</span> as <span class="math inline">\(f_\theta\)</span> w.r.t <span class="math inline">\(\nu\)</span>. There exist <span class="math inline">\(\{c_i\}_{i=1}^\infty\)</span> sequence of positive numbers and <span class="math inline">\(\{\theta_i\}_{i=1}^\infty\)</span> sequence in <span class="math inline">\(\cal \Theta\)</span> such that <span class="math inline">\(\sum_i c_i=1\)</span> and <span class="math inline">\(P_\theta \ll \sum c_iP_{\theta_i}\)</span> for all <span class="math inline">\(\theta \in \cal \Theta\)</span>. There exists a random variable <span class="math inline">\(X_0\)</span> such that <span class="math inline">\(P_{X_0}=\sum_i c_iP_{\theta_i}\)</span>. Furthermore, by MCT we can show <span class="math display">\[\frac{dP_{X_0}}{d\nu}=\sum_i c_i\frac{dP_{\theta_i}}{d\nu}=\sum_i c_i f_{\theta_i}.\]</span> Hence we have <span class="math display">\[\frac{dP_\theta}{dP_{X_0}}=\frac{f_{\theta}}{\sum_i c_i f_{\theta_i}}.\]</span>
Then part side of the factorization theorem tells us that <span class="math inline">\(f_\theta(x)=g(\theta,T(x))h(x)\)</span> implies <span class="math inline">\(T(X)\)</span> is sufficient for <span class="math inline">\(P_X\)</span>. That is to say, it requires to show that <span class="math inline">\(P_{X|T(X)=s}\)</span> is independent of <span class="math inline">\(\theta\)</span>. Similarly, the first step is to show that <span class="math display">\[\frac{dP_{X,T(X)}}{dP_{X_0,T(X_0)}}=\frac{dP_{X}}{dP_{X_0}}=\frac{f_{\theta}}{\sum_i c_i f_{\theta_i}}.\]</span> Then the result immediately follows by the condition and the same argument.</p>
</div>
<div class="remark">
<p><span id="unlabeled-div-39" class="remark"><em>Remark</em>. </span>In the proof of lemma 2.1, the author (Jun Shao) merely considered the case of finite measure <span class="math inline">\(\nu\)</span>. In fact, for a <span class="math inline">\(\sigma\)</span>-finite measure <span class="math inline">\(\nu\)</span>, we can find a finite measure <span class="math inline">\(\mu\)</span> (or a probability measure) that dominates <span class="math inline">\(\nu\)</span>. To show that, since <span class="math inline">\(\nu\)</span> is <span class="math inline">\(\sigma\)</span>-finite, we can find <span class="math inline">\(E_k\)</span> with finite measure such that <span class="math inline">\(\cup_k E_k\)</span> is the whole measurable space. Then the fact follows by defining <span class="math display">\[\mu(A)=\sum_{k=1}^\infty \frac{\nu(A\cap E_k)}{2^k\nu(E_k)}\]</span> for any <span class="math inline">\(A\)</span> in the corresponding <span class="math inline">\(\sigma\)</span>-algebra. Thus it suffices to consider the case of finite measure.</p>
</div>
</div>
<div id="markov-chains-and-martingales" class="section level3 hasAnchor" number="2.3.4">
<h3><span class="header-section-number">2.3.4</span> Markov chains and Martingales<a href="conditional-expectation.html#markov-chains-and-martingales" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-40" class="definition"><strong>Definition 2.10  </strong></span>A sequence of random vectors <span class="math inline">\(\{X_n:n\in\mathbb{N}\}\)</span> is said to be a (discrete time) Markov chain or Markov process if <span class="math display">\[P(B|X_1,\cdots,X_n)=P(B|X_n) \quad \mbox{a.s.}\]</span> for any <span class="math inline">\(B\in \sigma(X_{n+1}), n=2,3,\cdots\)</span></p>
</div>
<p>It can be seen that for a Markov chain <span class="math inline">\(\{X_n\}\)</span>, <span class="math inline">\(X_{n+1}\)</span> is conditionally independent of <span class="math inline">\((X_1,\cdots,X_{n-1})\)</span> given <span class="math inline">\(X_n\)</span>. We will list some equivalent conditions of Markov chain without proving below.</p>
<div class="proposition">
<p><span id="prp:unlabeled-div-41" class="proposition"><strong>Proposition 2.7  </strong></span>A sequence of random vectors <span class="math inline">\(X_n\)</span> is a Markov chain if and only if any one of the following three conditions holds.</p>
<ol style="list-style-type: decimal">
<li>For any integrable <span class="math inline">\(h(X_{n+1})\)</span> with Borel function <span class="math inline">\(h\)</span>, <span class="math inline">\(E[h(X_{n+1})|X_1,\cdots,X_n]=E[h(X_{n+1})|X_n]\)</span> a.s. for <span class="math inline">\(n\geq 2\)</span>.</li>
<li><span class="math inline">\(P(B|X_1,\cdots,X_n)=P(B|X_n)\)</span> a.s. for <span class="math inline">\(n\in \mathbb{N}\)</span> and <span class="math inline">\(B\in \sigma(X_{n+1},X_{n+2},\cdots)\)</span>.</li>
<li>For any <span class="math inline">\(n\geq 2\)</span>, <span class="math inline">\(A\in \sigma(X_1,\cdots,X_n)\)</span>, and <span class="math inline">\(B\in \sigma(X_{n+1},X_{n+2},\cdots)\)</span>, <span class="math inline">\(P(A\cap B|X_n)=P(A|X_n)P(B|X_n)\)</span> a.s.</li>
</ol>
</div>
<p>Further properties(periodic, invariant, irreducible, etc.) and applications of Markov chains like <strong>MCMC</strong> can be referred to chapter 4 in the textbook. Thereafter, we introduce <strong>martingale</strong> which is quite important in stochastic process and financial applications. Current studies of sequential analysis and game-theoretic statistics are also strongly connected with this topic.</p>
<div class="definition">
<p><span id="def:unlabeled-div-42" class="definition"><strong>Definition 2.11  </strong></span>The sequence of <span class="math inline">\(\{X_n,\cal F_n\}\)</span> with <span class="math inline">\(X_n\)</span> being a sequence of random variables defined on on a probability space <span class="math inline">\((\Omega, \cal F, P)\)</span> and <span class="math inline">\(\cal F_1\subset F_2\subset \cdots \subset F\)</span> being a sequence of <span class="math inline">\(\sigma\)</span>-fields (called a ``filtration’’) such that <span class="math inline">\(\sigma(X_n)\subset \cal F_n\)</span> is said to be a martingale if
<span class="math display">\[E(X_{n+1}|\cal F_n)=\rm X_n \quad \mbox{a.s.}\]</span> for all <span class="math inline">\(n\in \mathbb{N}\)</span>.</p>
</div>
<p>Furthermore, <span class="math inline">\(\{X_n,\cal F_n\}\)</span> is said to be a submartingale (supermartingale) if the “<span class="math inline">\(=\)</span>” of the formula is replaced by “<span class="math inline">\(\geq\)</span>” (or “<span class="math inline">\(\leq\)</span>”). We can derive that for a martingale <span class="math inline">\(X_n\)</span> (i) <span class="math inline">\(E(X_{n+j}|\cal F_n)=\rm X_n\)</span> a.s. ,and (ii) <span class="math inline">\(EX_1=EX_j\)</span> for all <span class="math inline">\(j=1,2,\cdots\)</span> by induction or iterating the formula in the definition. We say <span class="math inline">\(\{X_n\}\)</span> is a martingale (sub or super) if and only if <span class="math inline">\(\{X_n,\sigma(X_1,\cdots,X_n)\}\)</span> is a martingale (sub or super). In fact, since <span class="math inline">\(\sigma(X_n)\subset \cal F_n\)</span>, therefore <span class="math inline">\(\sigma(X_1,\cdots,X_n)=\sigma(\sigma(X_1)\cup\cdots\cup \sigma(X_n))\subset \cal F_n\)</span> and thus <span class="math inline">\(\sigma(X_1,\cdots,X_n)\)</span> is the smallest filtration <span class="math inline">\(\cal G_n\)</span> that satisfies <span class="math inline">\(\sigma(X_n) \subset \cal G_n\)</span> (or we say that <span class="math inline">\(X_n\)</span> is ``adapted’’ to <span class="math inline">\(\cal G_n\)</span>).</p>
<p>A construction of a martingale is to consider <span class="math inline">\(E(Y|\cal F_n)\)</span> for a integrable random variable <span class="math inline">\(Y\)</span> and a filtration <span class="math inline">\(\{\cal F_n\}\)</span> by the rule of conditional expectation. The following example is known as <strong>likelihood ratio martingale</strong>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 2.13  </strong></span>Consider a sequence of random variables <span class="math inline">\(\{X_n\}\)</span> from either <span class="math inline">\(P\)</span> or <span class="math inline">\(Q\)</span> that are measures on the space <span class="math inline">\((\Omega, \cal F)\)</span>. Let <span class="math inline">\(P_n\)</span> and <span class="math inline">\(Q_n\)</span> be <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> restricted to <span class="math inline">\(\cal F_n=\sigma(\rm X_1,\cdots,X_n)\)</span>. Suppose that <span class="math inline">\(Q_n \ll P_n\)</span> for each <span class="math inline">\(n\)</span>. Then <span class="math inline">\(\{L_n,\cal F_n\}\)</span> is a martingale where <span class="math inline">\(L_n=\frac{dQ_n}{dP_n}\)</span>. Moreover, suppose there exists a <span class="math inline">\(\sigma\)</span>-finite measure <span class="math inline">\(\nu_n\)</span> on <span class="math inline">\(\cal F_n\)</span> which is equivalent to <span class="math inline">\(P_n\)</span>. Then <span class="math inline">\(L_n=\frac{dQ_n}{d\nu_n}/\frac{dP_n}{d\nu_n}\)</span> is called the likelihood ratio.</p>
</div>
<p>The second example is known to be a simple random walk, which is a Markov chain as well as a martingale.</p>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 2.14  </strong></span>Let <span class="math inline">\(\{\epsilon_n\}\)</span> be a sequence of independent and integrable random variables. Let <span class="math inline">\(X_n=\sum_{i=1}^n \epsilon_i\)</span>. Then <span class="math inline">\(\{X_n\}\)</span> is a martingale since
<span class="math display">\[E(X_{n+1}|X_1,\cdots,X_n)=E(X_n+\epsilon_{n+1}|X_1,\cdots,X_n)=X_n.\]</span></p>
</div>
<p>The following theorem can be intermediately derived by Jensen’s inequality on conditional expectation, i.e., for convex function <span class="math inline">\(\phi\)</span>,<span class="math display">\[\phi(X_n)=\phi(E(X_{n+1}|\cal F_n))\leq \rm E(\phi(X_{n+1})|\cal F_n).\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-45" class="theorem"><strong>Theorem 2.5  </strong></span>Let <span class="math inline">\(\phi\)</span> be a convex function on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\{X_n,\cal F_n\}\)</span> is a martingale and <span class="math inline">\(\phi(X_n)\)</span> is integrable for all <span class="math inline">\(n\)</span>, then <span class="math inline">\(\{\phi(X_n),\cal F_n\}\)</span> is a submartingale.</li>
<li>If <span class="math inline">\(\{X_n,\cal F_n\}\)</span> is a submartingale and <span class="math inline">\(\phi(X_n)\)</span> is integrable for all <span class="math inline">\(n\)</span>, and <span class="math inline">\(\phi\)</span> is non-decreasing, then <span class="math inline">\(\{\phi(X_n),\cal F_n\}\)</span> is a submartingale.</li>
</ol>
</div>
<p>A well-known result of martingale comes from Doob’s decomposition which decompose any adapted stochastic process <span class="math inline">\(X_n\)</span> (i.e. <span class="math inline">\(\sigma(X_n)\subset \cal F_n\)</span> for all <span class="math inline">\(n\)</span>) into a martingale <span class="math inline">\(Y_n\)</span> and a predictable process <span class="math inline">\(Z_n\)</span> (i.e. <span class="math inline">\(Z_n\)</span> is measurable with respect to <span class="math inline">\(\cal F_n\)</span> for all <span class="math inline">\(n\)</span>). The following theorem is an extension of Doob’s decomposition.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-46" class="theorem"><strong>Theorem 2.6  </strong></span>Let <span class="math inline">\(\{X_n, \cal F_n\}\)</span> be a submartingale (supermartingale). Then <span class="math inline">\(X_n=Y_n+Z_n\)</span> for all <span class="math inline">\(n\)</span> where <span class="math inline">\(\{Y_n, \cal F_n\}\)</span> is a martingale, and <span class="math inline">\(Z_n\)</span> is an increasing (decreasing) sequence with <span class="math inline">\(EZ_n&lt;\infty\)</span> for all <span class="math inline">\(n\)</span>. Furthermore, if <span class="math inline">\(\mathrm{sup}_n E|X_n|&lt;\infty\)</span>, then <span class="math inline">\(\mathrm{sup}_n E|Y_n|&lt;\infty\)</span> and <span class="math inline">\(\mathrm{sup}_n EZ_n&lt;\infty\)</span>.</p>
</div>
<p>The decomposition can be constructed by letting <span class="math inline">\(Y_n=\sum_{i=1}^{n} \eta_i\)</span> and <span class="math inline">\(Z_n=\sum_{i=1}^{n} \xi_i\)</span> for <span class="math inline">\(\eta_i=X_i-X_{i-1}-E(X_i-X_{i-1}|\cal F_{i-1})\)</span>, <span class="math inline">\(\xi_i=E(X_i-X_{i-1}|\cal F_{i-1})\)</span> and <span class="math inline">\(\eta_1=\xi_1=0\)</span>. The last theorem in this chapter is the convergence theorem of martingales given by Doob.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-47" class="theorem"><strong>Theorem 2.7  </strong></span>Let <span class="math inline">\(\{X_n, \cal F_n\}\)</span> be a submartingale. If <span class="math inline">\(c:=\sup_n E|X_n|&lt;\infty\)</span>, then <span class="math inline">\(\lim_{n\to \infty}X_n \to X\)</span> a.s., where <span class="math inline">\(X\)</span> is a random variable satisfying <span class="math inline">\(E|X|\leq c\)</span>.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="integration-and-differentiation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="asymptotical-theory.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/01-prob.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
