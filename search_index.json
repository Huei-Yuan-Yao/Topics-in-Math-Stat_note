[["prob.html", "Chapter 2 Measure-based Probability theory", " Chapter 2 Measure-based Probability theory In this chapter, we quickly review the concepts in measure-based probability theory. The measure theory help us rigorously and axiomatically describe the randomness. When it comes to talk about a “probability” of an event, first we have to know about the underlying probability space. "],["probability-spaces-and-random-elements.html", "2.1 Probability Spaces and Random Elements", " 2.1 Probability Spaces and Random Elements 2.1.1 Recap: Measure theory We start from some basic concepts in the classical measure theory and use them to describe the probability space. Definition 2.1 ( \\(\\sigma\\)-algebra) Let \\(\\cal F\\) be a collection of subsets of a sample space \\(\\Omega\\). \\(\\cal F\\) is called a \\(\\sigma\\)-algebra or \\(\\sigma\\)-field on \\(\\Omega\\) if The empty set \\(\\phi \\in \\cal F\\). If \\(A \\in \\cal F\\), then \\(A^c \\in \\cal F\\). If \\(A_i \\in \\cal F\\) for \\(i=1,2,\\cdots\\), then \\(\\cup_{i=1}^{\\infty} A_i \\in \\cal F\\). We denote the smallest sigma algebra containing a collection \\(C\\) as \\(\\sigma(C)\\). In particular, we denote \\(\\cal B= \\sigma(C)\\) the \\(\\sigma\\)-field (called Borel field) where \\(C\\) denotes all finite open interval on \\(\\mathbb{R}\\). By Linderberg’s covering lemma, we can see \\(\\cal B\\) is also the smallest \\(\\sigma\\)-field containing the collection of all open sets on \\(\\mathbb{R}\\). Furthermore, we denote \\(\\cal B^k\\) as the Borel field in \\(\\mathbb{R}^k\\). Define the pair \\((\\Omega, \\cal F)\\) a measurable space if \\(\\cal F\\) is a \\(\\sigma\\)-field on \\(\\Omega\\). Definition 2.2 (Measure) A set function \\(\\nu(\\cdot)\\) defined on a \\(\\sigma\\)-field \\(\\cal F\\) is a measure if \\(0 \\leq \\nu(A) \\leq \\infty\\) for any \\(A \\in \\cal F\\). \\(\\nu(\\phi)=0\\) If \\(A_i \\in \\cal F\\), \\(i=1,2,\\cdots\\) and \\(A_i\\)’s are disjoint for any \\(i\\neq j\\), then \\[\\nu(\\cup_i A_i)=\\sum_i \\nu(A_i)\\] The triple \\((\\Omega, \\cal F, \\nu)\\) is called a measure space and it is called a probability space if \\(\\nu(\\Omega)=1\\). Proposition 2.1 Let \\((\\Omega,\\cal F,\\nu)\\) be a measure space. If \\(A \\subset B \\in \\cal F\\), then \\(\\nu(A)\\leq \\nu (B)\\). For any sequence \\(A_1,A_2,\\cdots \\in \\cal F\\), \\[\\nu(\\cup_i A_i)\\leq \\sum_i \\nu(A_i).\\] If \\(A_1 \\subset A_2 \\subset \\cdots \\in \\cal F\\) (or \\(A_1 \\supset A_2 \\supset \\cdots\\) and \\(\\nu(A_k)&lt;\\infty\\) for some \\(k\\in \\mathbb{N}\\)), then \\[\\nu(\\lim_{k \\to \\infty}A_k)=\\lim_{k\\to \\infty}\\nu(A_k).\\] Below we see some uniqueness theorem about the measure, or the result of the well known Dinkin’s \\(\\pi\\)-\\(\\lambda\\) theorem. Definition 2.3 A collection \\(C\\) of some subsets of \\(\\Omega\\) is a \\(\\pi\\)-system if it is closed under intersection. Definition 2.4 Let \\(C\\) be a collection of some subsets of \\(\\Omega\\). A measure \\(\\mu\\) is \\(\\sigma\\)-finite on \\(C\\) if there exist a sequence of sets \\(\\{A_k\\}\\) in \\(C\\) such that \\(\\Omega=\\cup_k A_k\\) and \\(\\mu(A_K)&lt;\\infty\\) for all \\(k\\). Lebesgue measure is an example of \\(\\sigma\\)-finite measure on \\(\\cal B\\). Since probability measure is always finite, so it is \\(\\sigma\\)-finite. Theorem 2.1 (Theorem 10.3 in Billingsley, 1986) Suppose that \\(\\mu_1\\) and \\(\\mu_2\\) are measures on \\(\\sigma(\\cal P)\\), where \\(\\cal P\\) is a \\(\\pi\\)-system. Suppose they are both \\(\\sigma\\)-finite and agree on \\(\\cal P\\), then they also agree on \\(\\sigma(\\cal P)\\). Example 2.1 Here are some examples using the uniqueness theorem: If \\(\\mu\\) is a measure defined on \\(\\cal B\\) and \\(\\mu([a,b])=b-a\\), then by checking that \\(\\cal P:=\\{[a,b]\\,|-\\infty&lt;a&lt;b&lt;\\infty\\}\\) is a \\(\\pi\\)- system and \\(\\cal B=\\sigma(\\cal P)\\) we can show that \\(\\mu\\) agrees with the Lebesgue measure on \\(\\cal B\\). (Billingsley, pp.225-236, 1986) Given measurable spaces \\((\\Omega_i, \\cal F_i,\\nu_i)_{i=1}^k\\) with \\(\\sigma\\)- finite measures, there exists a unique \\(\\sigma\\)- finite measure on product \\(\\sigma\\)- field \\(\\sigma(\\prod_\\limits{i=1}^k \\cal F_i)\\), which is known as product measure, defined by \\[\\nu_1\\times\\cdots\\times\\nu_k\\,(A_1\\times\\cdots\\times A_k):=\\prod_\\limits{i=1}^k \\nu_i(A_i)\\] for all \\(A_i \\in \\cal F_i,\\,i=1,\\cdots,k\\). 2.1.2 Measurable Functions Definition 2.5 (Measurable functions) Let \\(f\\) be a function from \\((\\Omega,\\cal F)\\) to \\((\\Lambda, \\cal G)\\) (both are measurable spaces). Then \\(f\\) is called a measurable function if \\(f^{-1}(\\cal G) \\subset \\cal F\\), where \\(f^{-1}(\\cal G):=\\{f^{-1}(A)\\,|\\, A\\in \\cal G\\}.\\) In the other words, preimage of any set in \\(\\cal G\\) of a measurable fucntion also lies in \\(\\cal F\\). If \\(\\Lambda=\\mathbb{R}\\) and \\(\\cal G=\\cal B\\), \\(f\\) is called a Borel function. Furthermore, a Borel function defined on a probability space is called a random Variable (a random vector with respect to (\\(\\mathbb{R}^k,\\cal B^k\\))). It is worthy noting that we may not require \\(\\cal F\\) to be the \\(\\sigma\\)- field on which \\(f\\) is measurable. In particular, it is easy to prove that \\(f^{-1}(\\cal G)\\) is also a \\(\\sigma\\)- field. This “smaller \\(\\sigma\\)- field is called \\(\\sigma\\)- field induced by \\(f\\) and we denote it by \\(\\sigma(f)\\). We omit the technical proof of the measurability of a function under fundamental operation such as sup, inf, limsup, liminf. Indicator functions, continuous functions, composition of measurable functions are also measurable functions. Readers can refer to the textbook for the proof. Proposition 2.2 (Approximation property) Suppose that f is measurable from \\((\\Omega,\\cal F)\\) to \\((\\bar{\\mathbb{R}}, \\bar{\\cal B})\\), where \\(\\bar{\\mathbb{R}}\\) is the extended real line with respect to \\(\\bar{\\cal B}\\). First assume \\(f \\geq 0\\), then there exists a positive and monotone sequence of simple functions \\(\\{f_n\\}\\) such that \\(f_n \\to f\\). For general measurable function \\(f\\), consider \\(f=f^{+}-f^{-}\\) for general function \\(f\\) and similar result goes. Below we show a lemma which let us connect two measurable function. Lemma 2.1 (Theorem A.42 in Schervish, 1995) Let \\(Y\\) be measurable from \\((\\Omega,\\cal F)\\) to \\((\\Lambda_Y,\\cal G_Y)\\) and \\(Z\\) be measurable from \\((\\Omega,\\cal F)\\) to \\((\\Lambda_Z,\\cal G_Z)\\). Define \\(T=\\{Y(\\omega),\\omega \\in \\Omega\\} \\subset \\Lambda_Y\\). Then \\(Z\\) is measurable from \\((\\Omega,\\sigma(Y))\\) to \\((\\Lambda_Z,\\cal G_Z)\\) if and only if \\(Z=h \\circ Y\\) for some \\(h\\) that is measurable from \\((T,T\\cap \\cal G_Y)\\) to \\((\\Lambda_Z,\\cal G_Z)\\). Remark: Proof of the “if” side is relatively obvious. (WLOG, we can assume \\(\\Lambda_Y\\) is the range of \\(Y\\).) Proof (only if). At first, we show that for \\(\\omega_1,\\ \\omega_2 \\in \\Omega\\), \\(Y(\\omega_1)=Y(\\omega_2)\\) implies \\(Z(\\omega_1)=Z(\\omega_2)\\). Suppose that \\(Y(\\omega_1)=Y(\\omega_1)=a\\). Since \\(Z\\) is measurable with respect to \\(\\sigma(Y)\\), there exist \\(A \\in \\cal G_Y\\) such that \\(Y^{-1}(A) =Z^{-1}(\\{Z(\\omega_1)\\})\\). Clearly \\(\\omega_1,\\ \\omega_2 \\in Z^{-1}(\\{Z(\\omega_1)\\})\\), thus \\(Z(\\omega_1)=Z(\\omega_2)\\). By this first step, the function \\(h\\) with \\(h(Y(\\omega))=Z(\\omega)\\) is well defined with domain being the range of \\(Y\\). Secondly, we prove such \\(h\\) is measurable with respect to \\(T \\,\\cap\\, \\cal G_Y\\). Given \\(B \\in \\cal G_Z\\), let \\(A\\) be an event in \\(\\cal G_Y\\) such that \\(Y^{-1}(A)=Z^{-1}(B)\\) which exists since \\(Z\\) is measurable with respect to \\(\\sigma(Y)\\), then \\(h\\) is measurable if \\(h^{-1}(B)=A\\,\\cap\\,T\\), where \\(T\\) is the range of \\(Y\\). (\\(h^{-1}(B)\\subset A\\,\\cap\\,T\\)). Given \\(a \\in h^{-1}(B)\\), then \\(h(a) \\in B\\) and \\(a=Y(\\omega)\\) for some \\(\\omega\\in \\Omega\\) (\\(a \\in T\\)) by the domain of \\(h\\), and thus \\(Z(\\omega)=h(Y(\\omega)) \\in B\\), \\(\\omega \\in Z^{-1}(B)=Y^{-1}(A)\\) implies \\(a \\in A\\). (\\(A\\,\\cap\\,T\\subset h^{-1}(B)\\)). Given \\(a \\in A \\cap T\\), \\(a=Y(\\omega) \\in A\\) for some \\(\\omega \\in \\Omega\\), then \\(\\omega \\in Y^{-1}(A)=Z^{-1}(B)\\) and by definition of \\(h\\), \\(h(a)=Z(\\omega)\\in B\\), thus \\(a \\in h^{-1}(B)\\). "],["integration-and-differentiation.html", "2.2 Integration and Differentiation", " 2.2 Integration and Differentiation 2.2.1 Lebesgue integral An usual way to define Lebesgue integral is from simple function to non-negative function by approximation property, then to a general function by an easy decomposition. Let us start from simple function. Assume \\(\\phi=\\sum_\\limits{i=1}^k a_iI_{A_i}\\), \\(A_i\\)’s are disjoint. Then its integral with respect to measure \\(\\nu\\) is \\[\\int \\phi d\\nu=\\sum_\\limits{i=1}^k a_i\\nu(A_i).\\] Clearly \\(A_i\\) is required to be measruable which is equivalent to say \\(\\phi\\) is measurable. It can be seen such integration concept comes from “partition the range” while the Riemann integration comes from partition of the domain. This is also shown in the construction of the approximation property. In particular, we define \\(a\\infty=0\\) when \\(a=0\\) to deal with some special circumstances. For non-negative function we have two equivalent definition of integration. Definition 2.6 Let \\(f\\) be a non-negative Borel function and define its integral to be \\[\\int f d\\nu=\\underset{\\phi\\in S_f}{\\mbox{sup}} \\int \\phi d\\nu,\\] where \\(S_f\\) is the collection of all non-negative simple function satisfying \\(\\phi(\\omega) \\leq f(\\omega)\\) for any \\(\\omega \\in \\Omega\\). Another definition may be more suitable for operation, which comes from the well known Monotone Convergence Theorem. Definition 2.7 Let \\(f\\) be a non-negative Borel function and define its integral to be \\[\\int f d\\nu=\\lim_{n \\to \\infty} \\int f_n d\\nu,\\] where \\(0 \\leq f_n \\uparrow f\\) for \\(f_n\\) is simple function for all \\(n\\). For general function \\(f\\), its integral is defined as \\[\\int f d\\nu=\\int f_{+} d\\nu-\\int f_{-} d\\nu,\\] we say this integral exists if and only if both integral on the right hand side are finite. Furthermore, we say \\(f\\) is integrable if both integral are finite. Clearly, we have \\(f\\) is integrable if and only if \\(|f|\\) is since \\(|f|=f_{+}+f_{-}\\). Below are some basic proposition: Proposition 2.3 Let \\(f\\) ang \\(g\\) are Borel function. Then If \\(f \\leq g\\) and \\(a \\in \\mathbb{R}\\), then \\(\\int (af)\\, d\\nu\\) exists and is equal to \\(a\\int f \\, d\\nu\\). If both \\(\\int f \\, d\\nu\\) and \\(\\int g \\, d\\nu\\) exist and \\(\\int f \\, d\\nu+\\int g \\, d\\nu\\) is well defined (not \\(\\infty-\\infty\\)), then \\(\\int (f+g) \\, d\\nu\\) exists and is eual to \\(\\int f \\, d\\nu+\\int g \\, d\\nu\\). If \\(f \\leq g\\) a.e., then \\(\\int f \\, d\\nu \\leq \\int g \\, d\\nu\\) if the integrals exist. If \\(f \\geq 0\\) a.e. and \\(\\int f d\\nu =0\\), then \\(f=0\\) a.e. \\(\\nu(A)=0\\) implies that \\(\\int_A f d\\nu =0\\) where \\(\\int_A f d\\nu := \\int fI_A d\\nu\\). Here we also recall some classic theorem about limit and integral without proof in the next proposition. Proposition 2.4 Let \\(f_1, f_2,\\cdots,\\) be a sequence of Borel functions on \\((\\Omega,\\cal F,\\nu)\\). (Monotone convergence theorem). If \\(0 \\leq f_1 \\leq f_2 \\leq \\cdots\\) and \\(\\lim_\\limits{n \\to \\infty} f_n=f\\) a.e., then \\(\\int \\lim_\\limits{n \\to \\infty} f_n d\\nu=\\lim_\\limits{n \\to \\infty} \\int f_n d\\nu\\). (Dominated convergence theorem). If \\(\\lim_\\limits{n \\to \\infty} f_n=f\\) a.e. and there exists an integrable function \\(g\\) such that \\(|f_n| \\leq g\\) a.e., then \\(\\int \\lim_\\limits{n \\to \\infty} f_n d\\nu=\\lim_\\limits{n \\to \\infty} \\int f_n d\\nu\\). (Fatous’s lemma). If \\(f_n \\geq 0\\), then \\(\\int \\lim_\\limits{n \\to \\infty} f_n d\\nu=\\lim_\\limits{n \\to \\infty} \\int f_n d\\nu\\). Example 2.2 Here we consider the interchange of differentiation and integration. That is, for fixed \\(\\theta \\in \\mathbb{R}\\), let \\(f(\\omega, \\theta)\\) be a Borel function on \\((\\Omega,\\cal F,\\nu)\\). Assume that \\(\\partial f(\\omega, \\theta)/\\partial \\theta\\) exists a.e. for \\(\\theta \\in (a,b) \\subset \\mathbb{R}\\) and that \\(|\\partial f(\\omega, \\theta)/\\partial \\theta| \\leq g(\\omega)\\) a.e., where \\(g\\) is an integrable function on \\(\\Omega\\). Then for each \\(\\theta \\in (a,b)\\), \\(\\partial f(\\omega, \\theta)/\\partial \\theta\\) is integrable and by mean value theorem and Dominated convergence theorem, we have \\[\\frac{d}{d\\theta} \\int f(\\omega, \\theta) d\\nu= \\int (\\partial f(\\omega, \\theta)/\\partial \\theta) \\, d\\nu.\\] Example 2.3 Consider the moment generating function of a random variable \\(X\\), \\(M(t)\\) on a finite interval \\((a,b)\\). By the above example and the fact that \\(|x|e^{t_0dx} \\leq c_{+} e^{(t_0+\\delta) x}+c_{-} e^{(t_0-\\delta) x}\\), where \\(c_{+}=\\underset{x \\geq 0}{\\max} \\frac{|x|e^{tx}}{e^{(t_0+\\delta)x} }\\) and \\(c_{-}=\\underset{x \\leq 0}{\\max} \\frac{|x|e^{tx}}{e^{(t_0-\\delta)x} }\\) for some \\(t_0 \\in (a,b)\\), then we have \\(M&#39;(t)=E(Xe^{tX})\\). Theorem 2.2 (Change of variables) Let \\(f\\) be measurable from \\((\\Omega,\\cal F,\\nu)\\) to \\((\\Lambda,\\cal G)\\) and \\(g\\) be Borel on \\((\\Lambda,\\cal G)\\). Then \\[\\int_{\\Omega} g(f(\\omega)) d\\nu(\\omega)= \\int_{\\Lambda} g(x) d(\\nu \\circ f^{-1})(x),\\] where \\(\\nu \\circ f^{-1}(B):= \\nu(f^{-1}(B))\\) for \\(B \\in \\cal G\\). Consider an easy case, let \\(g=\\sum_\\limits{i=1}^k c_iI_{A_i}\\) for \\(A_1,\\cdots, A_k\\) disjoint. Then the right hand side is equal to \\(\\sum_\\limits{i=1}^k c_i\\, \\nu\\circ f^{-1}(A_i)\\) and let \\(B_i=f^{-1}(A_i)\\), then the equality holds clearly. An important application of the theorem is that for random variable X with distribution \\(P \\circ X^{-1}\\), we have \\[ E(g(X))=\\int g(X(\\omega)) dP(\\omega)=\\int g(x) dP\\circ X^{-1}(x).\\] Also, by the uniqueness theorem of measure, \\(P\\circ X^{-1}\\) coincides with the cumulative density function \\(F(x):=P(X\\leq x)\\). We also denote \\(P\\circ X^{-1}\\) as \\(P_X\\), the distribution of \\(X\\). Below we consider the interchange of integration which is known as Fubini’s Theorem. Theorem 2.3 (Fubini Theorem) Let \\(\\nu_i\\) be \\(\\sigma\\)-finite measure on \\((\\Omega_i,\\cal F_i)\\) for \\(i=1,2\\), and let \\(f\\) be Borel function on the product \\(\\sigma\\) algebra. Suppose \\(f \\geq 0\\) (w.r.t. Tonelli’s theorem) or \\(f\\) integrable with respect to \\(\\nu_1 \\times \\nu_2\\). Then \\[\\int_{\\Omega_1} f(\\omega_1,\\omega_2) d\\nu_1\\] exists \\(\\nu_2\\)-a.e. and is a Borel function on \\(\\Omega_2\\) whose integral with respect to \\(\\nu_2\\) exists, and \\[\\int_{\\Omega_1 \\times \\Omega_2} f(\\omega_1,\\omega_2) d(\\nu_1\\times \\nu_2)=\\int_{\\Omega_2}(\\int_{\\Omega_1} f(\\omega_1,\\omega_2) d\\nu_1) d\\nu_2.\\] The “\\(\\sigma\\)-finite” condition on the measures is assumed for the uniqueness of \\(\\nu_1 \\times \\nu_2\\). It can be seen necessary if we consider the interchange of integral of an indicator function on which the set is an arbitrary set in the product field. In the following We discuss the coincidence of Riemann integral and Lebesgue integral. An easy result can be shown on finite interval \\((a,b)\\) that Riemann integrability implies Lebesgue integrability and the values of the integrals coincides. That is, \\[\\int_{(a,b)} f d\\lambda=\\int_a^b f(x) dx. \\] Furthermore, we can extend the result to the domain like \\((0,\\infty)\\) for a positive function \\(f\\) by the monotone convergence theorem. We end this chapter by the two classical examples. The first one is used for calculating the expectation of a positvie random variable. Example 2.4 Combining the result of Fubini’s Theorem and the above consequence, we can show that for \\(X&gt; 0\\), \\[E(X)=\\int_0^{\\infty}(1-F(t)) \\, dt.\\] Let the \\(\\lambda\\) be Lebesgue measure, the RHS can be written as \\(\\int_{(0,\\infty)} \\int I_{(t, \\infty)}(x) dP_X(x) \\, d\\lambda(t)\\), then the result follows by applying Fubini’s Theorem. Example 2.5 For function \\(f \\geq 0\\) taken values on \\(\\Omega=(\\omega_1,\\omega_2,\\cdots)\\), by monotone convergence theorem and the fact that \\(f=\\lim_\\limits{n\\to \\infty} \\sum_\\limits{i=1}^n f(\\omega_i)\\) we have \\(\\int f d\\nu= \\sum_i f(\\omega_i)\\nu(\\omega_i)\\). 2.2.2 Radon-Nikodym Derivative Definition 2.8 (absolutely continuous) Given two measures \\(\\mu,\\nu\\) on \\((\\Omega, \\cal F)\\), we say \\(\\mu\\) is absolutely continuous with respect to \\(\\nu\\), denoted by \\(\\mu \\ll \\nu\\), if \\(\\nu(A)=0\\) implies \\(\\mu(A)=0\\) for any \\(A \\in \\cal F\\). Theorem 2.4 (Rando-Nikodym Theorem) Given two measures \\(\\mu,\\nu\\) on \\((\\Omega, \\cal F)\\) and \\(\\nu\\) is \\(\\sigma\\)-fintie. If \\(\\mu \\ll\\nu\\), then there exists a nonnegative Borel function \\(f\\), which is unique \\(\\nu\\)-a.e. on \\(\\Omega\\) such that \\[\\mu(A)=\\int_A f d\\nu.\\] Such function \\(f\\) is called a Rando-Nikodym derivative or density and is denote by \\(\\frac{d\\mu}{d\\nu}\\). A function \\(f\\geq 0\\) \\(\\nu\\)-a.e. is called probabiilty density function (p.d.f.) w.r.t a probabiity measure \\(\\mu\\) if \\(\\int f d\\nu=1\\). A discrete p.d.f. is a p.d.f w.r.t counting measure and a Lebesgue p.d.f corresponds to Lebesgue measure. A sufficient and necessary condition for a c.d.f. \\(F\\) having a Lebesgue p.d.f is that \\(F\\) is absolutely continuous. Below we consider a special case that both Lebesgue p.d.f and discrete p.d.f cannot be well defined. Example 2.6 Suppose that \\(Z\\) is a standard normal r.v. and \\(X=ZI_{[1,\\infty]}(Z)\\), clearly \\(X\\) has no Lebesgue density since \\(P_X({0})\\neq 0\\). Let \\(\\mu\\) be the probability measure on \\((\\mathbb{R},\\cal B)\\) such that \\(\\delta_0(A)=I_A(0)\\). First we claim that \\(P_X \\ll \\delta_0+\\lambda\\) since for any set \\(A\\in \\cal F\\), \\((\\delta_0+\\lambda)(A)=0\\) implies \\(0 \\notin A\\) and \\(\\lambda(A)=0\\). Then \\(P_X(A)=P(Z\\geq 1,Z \\in A)+P(Z&lt;1,Z \\in A)\\stackrel{0\\notin A}=P(Z\\geq 1,Z \\in A)\\stackrel{\\lambda(A)=0}=0,\\) which proves the claim. Secondly, we would like to find out the Radon-Nikodym derivative \\(\\frac{dP_X}{d(\\delta_0+\\lambda)}\\). The density with respect to \\((\\delta_0+\\lambda)\\) can be written as \\(\\Phi(0)I_{\\{0\\}}(x)+\\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}I_{[1,\\infty)}(x)\\), where \\(\\Phi(x)\\) is the c.d.f of standard normal. Remark. Firstly, It can be noticed that \\[\\int_A f\\, d\\delta_0=\\int f(0)I_{A}(x)\\, d\\delta_0(x)=f(0)I_A(0).\\] The first equality holds since \\(fI_{A}(x)=f(0)I_{A}(x)\\) \\(\\delta_0\\)-a.e. Secondly, for the “overlapping case” such as consider the Randon-Nykodym derivative of \\(ZI_{[1,\\infty)}(z)+2I{(0,1)}(z)\\), the density is \\(\\phi(x)I_{(1,\\infty)\\setminus \\{2\\}}+P(X=2)I_2(x)+P(X=0)I_0(x)\\). It is important that the set w.r.t first component has to consider minusing \\(\\{2\\}\\). Below we list some propositions regarding to Randon-Nykodym derivative. Proposition 2.5 Let \\(\\nu\\) be a \\(\\sigma\\)-finite measure on a measure space \\((\\Omega,\\cal F)\\). Then If \\(\\lambda\\) is a measure, \\(\\lambda \\ll \\nu\\), and \\(f\\geq 0\\), then \\[\\int f d\\lambda=\\int f\\frac{d\\lambda}{d\\nu} d\\nu\\] If \\(\\lambda_i\\ll \\nu\\) for \\(i=1,2\\), then \\(\\lambda_1+\\lambda_2\\ll \\nu\\) and \\[\\frac{d(\\lambda_1+\\lambda_2)}{d\\nu}=\\frac{d\\lambda_1}{d\\nu}+\\frac{d\\lambda_2}{d\\nu} \\quad \\nu\\mbox{-a.e.}\\] If \\(\\tau\\) is a measure, \\(\\lambda\\) is a \\(\\sigma\\)-finite measure, and \\(\\tau\\ll\\lambda\\ll \\nu\\), then \\[\\frac{d\\tau}{d\\nu}=\\frac{d\\tau}{d\\lambda}\\frac{d\\lambda}{d\\nu}\\quad \\nu\\mbox{-a.e.}\\] In particular, if \\(\\lambda\\ll \\nu\\) and \\(\\nu \\ll \\lambda\\) (equivalent), then \\(\\frac{d\\lambda}{d\\nu}=(\\frac{d\\nu}{d\\lambda})^{-1}\\). The first result can be quickly verified by utilizing the approximation property and Monotone Convergence Theorem. The third result (chain rule) can be directly obtained by the first one. Below we consider the density of transformation of random variables. A general result is given in proposition 1.8 of the textbook. Example 2.7 Suppose that \\(X\\) is a random variable with Lebesgue p.d.f. \\(f_X\\) and \\(f_X(x)=0\\) for \\(x\\leq 0\\). Let \\(Y=X^2\\) and \\[g(y)=(2\\sqrt{y})^{-1}f_X(\\sqrt{y})I_{(0,\\infty)}(y).\\] Then \\(g\\) is a Lebesgue p.d.f. of \\(Y\\). To verify the above result, i.e. we want \\(P(Y\\in A)=\\int_A g(y) d\\lambda(y)\\) for \\(A \\in \\cal B(\\mathbb{R})\\). It suffices to consider the case of intervals like \\((-\\infty,b]\\) (a \\(\\pi\\)-system) and let \\(\\lambda^{+}(A):=\\int_A I_{(0,\\infty)}(y) d\\lambda(y)\\) (\\(\\frac{d\\lambda^{+}}{d\\lambda}(y)=I_{(0,\\infty)}(y)\\)) and \\(h(y)=\\sqrt{y}I_{(0,\\infty)}(y)\\). Then for \\(b&gt;0\\), \\[\\begin{split} \\int_{(-\\infty,b]} g(y) d\\lambda(y)&amp;=\\int_{(-\\infty,b]} (2\\sqrt{y})^{-1}f_X(\\sqrt{y})I_{(0,\\infty)}(y) d\\lambda(y) \\\\ &amp;= \\int_{(-\\infty,b]} (2\\sqrt{y})^{-1}f_X(\\sqrt{y}) \\frac{d\\lambda^{+}}{d\\lambda}(y) d\\lambda(y) \\\\ &amp;=\\int I_{(0,b)}(y) (2\\sqrt{y})^{-1}f_X(\\sqrt{y}) d\\lambda^{+}(y)\\\\ &amp;=\\int I_{(0,\\sqrt{b})}(z) (2z)^{-1}f_X(z) d(\\lambda^{+}\\circ h^{-1})(z). \\end{split}\\] Note that \\[\\lambda^{+}\\circ h^{-1}((-\\infty,b))=\\lambda^{+}((0,b^2))=b^2=\\int_{(-\\infty,b)}2xI_{(0,\\infty)}(x) d\\lambda(x).\\] Thus \\[\\frac{d\\lambda^{+}\\circ h^{-1}}{d\\lambda}(x)=2xI_{(0,\\infty)}(x).\\] Therefore by applying the first one in proposition 2.5 again, we can derive that \\[\\int_{(-\\infty,b]} g(y) d\\lambda(y)=\\int_{(0,\\sqrt{b})}f_X(z) d\\lambda(z)=P_X((0,\\sqrt{b}))=P(Y\\in (0,b]).\\] For the case \\(f_x \\neq 0\\) on \\((-\\infty,0)\\), the RN (Radon-Nykodim derivative) is \\[ f_Y(y)=(\\frac{f_x(\\sqrt y)}{(2\\sqrt y)}+\\frac{f_x(-\\sqrt y)}{(2\\sqrt y)})I_{(0,\\infty)}(y).\\] In summary, the proof is mainly based on (i) the change of measure and (ii) the integral formula w.r.t change of variables. Remark. Can the result above be generalized to a general measure other than Lebesgue measure? "],["conditional-expectation.html", "2.3 Conditional Expectation", " 2.3 Conditional Expectation 2.3.1 Conditional Expectation Definition 2.9 Let \\(X\\) be a integrable random variable on \\((\\Omega, \\cal F,P)\\). Let \\(\\cal A\\) be a sub \\(\\sigma\\)-field of \\(\\cal F\\). The condition expectation of \\(X\\) given \\(\\cal A\\) (which we denote it as \\(E(X|\\cal A)\\)) is the a.s.-unique random variable satisfying \\(E(X|\\cal A)\\) is measurable from \\((\\Omega,\\cal A)\\) to \\((R,\\cal B)\\) and \\(\\int_A E(X|\\cal A) \\rm dP=\\) \\(\\int_{A} X dP\\) for all \\(A \\in \\cal A\\). Let \\(B \\in \\cal F\\). The conditional probability of \\(B\\) given \\(\\cal A\\) is defined to be \\(P(B|\\cal{A})=\\) \\(E(I_B|\\cal A)\\). Let \\(Y\\) be measurable from \\((\\Omega, \\cal F,P)\\) to \\((\\Lambda, \\cal G)\\). The conditional expectation of \\(X\\) given \\(Y\\) is defined to be \\(E(X|Y)=E(X|\\sigma(Y))\\). Define \\(\\mu^{+}=\\int_A X^{+} dP\\) for \\(A \\in \\cal A\\), then such \\(\\mu^{+}\\) is a measure on \\(\\cal A\\). Let \\(P_0\\) be the restriction of \\(P\\) on \\(\\cal A\\). Then clearly we have \\(\\mu^{+}\\ll P_0\\). It is easy to check that \\(E(X^{+}|\\cal A)= \\frac{d\\mu^{+}}{dP_0}\\) and \\(E(X^{-}|\\cal A)= \\frac{d\\mu^{-}}{dP_0}\\) (by similarly defining \\(\\mu^{-}\\)) will satisfy the definition of conditional expectation. The uniqueness and existence follows by RN theorem. Example 2.8 Suppose \\(\\Omega={1,2,3,4}\\) and \\(P(\\{k\\})=\\frac{1}{4}\\) for \\(k\\in\\Omega\\). Suppose that \\(X(k)=k\\) for \\(k\\in\\Omega\\). Let \\(Y(1)=4,\\ Y(2)=5,\\ Y(3)=Y(4)=6\\). Find \\(E(X|\\sigma(Y))=h(Y)\\). Directly by calculating that \\(\\int_A E(X|\\cal A) \\rm dP=\\) \\(\\int_{A} X dP\\), we can derive that \\(h(4)=1,\\ h(5)=2,\\ h(6)=\\frac{7}{2}\\). If we consider trivial \\(\\sigma\\)-algebra \\(\\cal A=\\{\\phi,\\ \\Omega\\}\\), then by measurability we know that \\(E(X|\\cal A)\\) must be a constant function. By the integral restriction, clearly \\(E(X|\\cal A)=\\rm E(X)\\). Furthermore, suppose \\(X\\) is measurable w.r.t \\(\\cal A_0\\) which is a sub \\(\\sigma\\)-field of \\(\\cal A\\). Then \\(E(X|\\cal A)=\\rm X\\). Note that \\(E(X|\\sigma(Y))=E(Y)=h(Y)\\). Thus we may write \\(E(X|Y=y)=h(y)\\). Below we give some proposition regarding to conditoinal expectation. Proposition 2.6 Given the same setup above, we have If \\(X=c\\) a.s. for \\(c \\in \\mathbb{R}\\) then \\(E(X|\\cal A)=c\\) a.s. (measurability is trivial for a constant function). If \\(X\\leq Y\\) a.s., then \\(E(X|\\cal A)\\leq \\rm E(Y|\\cal A)\\) a.s. (which can be quickly proved by linearity) For \\(E|X|,\\ E|Y|&lt; \\infty\\), \\(E(aX+bY|\\cal A)=a\\rm E(X|\\cal A)+b\\rm E(Y|\\cal A)\\). \\(E(E(X|\\cal A))= \\int_{\\Omega} \\rm E(X|\\cal A) \\rm dP=\\int_{\\Omega} X dP=E(X)\\). Let \\(\\cal A_0 \\subset \\cal A\\) be sub \\(\\sigma\\)-fields of some \\(\\cal F\\). Then \\(E(E(X|\\cal A)|\\cal A_0))=\\rm E(\\rm E(X|\\cal A_0)|\\cal A))=\\rm E(X|\\cal A_0)\\). If \\(\\sigma(Y)\\subset \\cal A\\) and \\(E(|XY|)&lt; \\infty\\), then \\(E(XY|\\cal A)=\\rm YE(X|\\cal A)\\). Suppose \\(X\\) and \\(Y\\) are independent, \\(g\\) is Borel function and \\(E|g(X,Y)|&lt;\\infty\\). Let \\(h(y)=E(g(X,y))\\) for all \\(y \\in Y\\). Then \\(E(g(X,Y)|Y)=h(Y)\\), or equivalently, \\(E(g(X,Y)|Y=y)=h(y)\\). If \\(E(X^2)&lt; \\infty\\), then \\([E(X|\\cal A)]^2\\leq \\rm E(X^2|\\cal A)\\) a.s. Remark. We briefly discuss how to show some of the properties as following: For 6. we start from considering \\(Y\\) is a simple function and use LDCT on general measurable \\(Y\\). For 7. which consider the independence, For 8., we directly show that by \\(0 \\leq E[X-E(X|\\cal A)^2|\\cal A]=\\rm E(X^2|\\cal A)- \\rm (E(X|\\cal A))^2\\). The equality follows by linearity and 6. Example 2.9 This example shows that \\(E(X|\\cal A)\\) is the best guess of \\(X\\) given some knowledge of \\(\\cal A\\), which means \\[\\int (X-E(X|\\cal A))^2 \\rm dP\\leq \\int (X-Y)^2 \\rm dP\\] for any \\(Y\\) measurable w.r.t. \\(\\cal A\\). Let \\(Z=Y-E(X|\\cal A)\\) measurable w.r.t. \\(\\cal A\\). It follows by that \\(\\int Z(X-E(X|\\cal A)) \\rm dP=E(E(Z(X-E(X|\\cal A))|\\cal A))=0\\). 2.3.2 Independence First we extend the definition of independence to \\(\\sigma\\)-algebra. Let \\((\\Omega,\\cal F,P)\\) be a probability space. Let \\(\\cal C\\) be a collection of subsets in \\(\\cal F\\). Events in \\(\\cal C\\) is said to be independent if for any \\(n \\in \\mathbb{N}\\) and distinct events \\(A_1,\\cdots,A_n\\) in \\(\\cal C\\), we have \\(P(A_1\\cap\\cdots A_n)=\\prod_{i=1}^n P(A_i)\\). Collections \\(\\cal C_i\\subset \\cal F,\\, i\\in I\\) are said to be independent if events in any collection of the form \\(\\{A_i\\in\\cal C_i:i\\in I\\}\\) are independent. Random elements \\(X_i\\) are independent if \\(\\sigma(X_i)\\) are independent. Suppose that \\(X\\) is a random variable on \\((\\Omega,\\cal F,P)\\) with finite moment and \\(\\cal A_1\\) and \\(\\cal A_2\\) are sub \\(\\sigma\\)-fields of \\(\\cal F\\). If \\(\\sigma(\\sigma(X)\\cup\\cal A_1)\\) and \\(\\cal A_2\\) are independent, then \\[E(X|\\sigma(\\cal A_1\\cup \\cal A_2))=\\rm E(X|\\cal A_1)\\quad a.s.\\] In fact, it is sufficient to show that \\[\\int_{\\cal A_1\\cap \\cal A_1}E(X|\\cal A_1)\\rm dP= \\int_{\\cal A_1\\cap \\cal A_1}X dP.\\] for any \\(A_1\\in \\cal A_1\\) and \\(A_2\\in \\cal A_2\\) since \\(\\mathbb{C}=\\rm \\{A_1\\cap A_2| A_1\\in \\cal A_1,A_2\\in \\cal A_2\\}\\) is a \\(\\pi\\)-system and \\(\\sigma(\\mathbb{C})=\\sigma(\\cal A_1\\cup \\cal A_2)\\). This result can be further established by the fact that \\(E(E(X|A_1) I_{A_2})=\\rm E(X|A_1)P(A_2)\\) given the assumption. As a special case, \\(E(X|Y_1,Y_2)=E(X|Y_1)\\) if \\(X\\) and \\((Y_1,Y_2)\\) are independent by the result of the exercise below (by replacing \\(\\cal A_1\\), \\(\\cal A_2\\) with \\(\\sigma(Y_1)\\) and \\(\\sigma(Y_1)\\)). Also, if \\(E|X|&lt;\\infty\\), \\(\\sigma(X)\\) and \\(\\sigma(Y)\\) are independent, then \\(E(X|Y)=E(X)\\) a.s. Exercise 2.1 Let \\(Z=(Y_1,Y_2)\\), \\(\\sigma(Z)\\stackrel{?}=\\sigma(\\sigma(Y_1)\\cup\\sigma(Y_2))\\). First we show that \\[\\sigma(\\sigma(Y_1)\\cup\\sigma(Y_2))=\\sigma(\\{Y_1^{-1}(B_1)\\cap\\sigma(Y_2^{-1}(B_2)):B_1\\in \\cal B^n, \\rm B_2\\in \\cal B^m\\}).\\] Then the result follows if \\[\\sigma(Z)=\\sigma(\\{Y_1^{-1}(B_1)\\cap\\sigma(Y_2^{-1}(B_2)):B_1\\in \\cal B^n, \\rm B_2\\in \\cal B^m\\}).\\] For the first equality, notice that \\(\\subseteq\\)-direction is clear since both \\(\\sigma(Y_1)\\) and \\(\\sigma(Y_2)\\) lie in the \\(\\sigma\\)-field on the right hand side. For another direction, note that for any \\(B_1\\) and \\(B_2\\), \\(Y_i^{-1}(B_i) \\in \\sigma(Y_1)\\cup \\sigma(Y_2),\\, i=1,2\\), thus the intersection must lie in the \\(\\sigma\\)-field on the left hand side. For second equality, the \\(\\supseteq\\)-direction is obvious since the set \\(\\mathbb{D}:=\\{Y_1^{-1}(B_1)\\cap\\sigma(Y_2^{-1}(B_2)):B_1\\in \\cal B^n, \\rm B_2\\in \\cal B^m\\}\\) is just \\(\\{Z^{-1}(B_1\\times B_2):B_1\\in \\cal B^n, \\rm B_2\\in \\cal B^m\\}\\). For another direction, note that for \\(\\cal B^{n+m}=\\sigma(\\cal B^n\\times \\cal B^m)\\) and the fact that \\(Z^{-1}(\\sigma(\\mathbb{D})) \\subseteq \\sigma(Z^{-1}(\\mathbb{D}))\\) (in fact, they are equal). The fact can be shown by proving the set \\(\\cal E:=\\{\\rm A| Z^{-1}(A)\\in \\sigma(Z^{-1}(\\mathbb{D}))\\}\\) is a \\(\\sigma\\)-field including the collection \\(\\mathbb{D}\\). Then the result follows from \\(\\sigma(\\mathbb{D})\\subseteq \\cal E\\). 2.3.3 Conditional Distribution First we define \\(\\mu(B,Y)=E(I_B(X)|Y)\\). In other words, \\(\\int_{Y^{-1}(C)} \\mu(B,Y)dP=\\int_{Y^{-1}(C)} I_B(X)dP\\). Furthermore, If \\[\\int I_C(y)[\\int_B f_{X|Y=y}(x) d\\mu(x)]dP_Y(y)=P((X,Y)\\in B\\times C)\\] for \\(B\\in \\cal B_X\\) and \\(C \\in \\cal B_\\rm{Y}\\), then we say \\(f_{X|Y=y}(x)\\) is the conditional density of \\(X\\) given \\(Y=y\\) w.r.t \\(\\mu\\). In fact, such function \\(\\mu(B,Y)\\) is called a random probability measure. Suppose \\((X,Y)\\) has a joint density function \\(f_{X\\times Y}\\) w.r.t \\(\\mu \\times \\nu\\). First of all, we show the result in basic probability that the conditional can be written as joint over marginal. Let \\(f_{X|Y=y}(x)=\\frac{f_{X\\times Y}(x,y)}{f_Y(y)}\\). The result can be validated by \\[\\int I_C(y)(\\int_B \\frac{f_{X\\times Y}(x,y)}{f_Y(y)}\\, d\\mu(x))dP_Y(y)=\\int_{B\\times C }f_{X\\times Y}(x,y) d(\\mu\\times \\nu)(x,y).\\] Secondly, we would like to show if \\(E(X|Y=y)=\\int xf_{X|Y=y}(x) d\\mu(x)\\). In other words, we may validate that \\[E[XI_C(y)]=\\int I_C(y) [\\int xf_{X|Y=y}(x) d\\mu(x)] dP_Y(y),\\] by the defintion, which can be quickly proved by approximating \\(X\\) with simple functions. 2.3.4 Markov chain and Martingale "],["asymptotical-theory.html", "2.4 Asymptotical Theory", " 2.4 Asymptotical Theory "]]
