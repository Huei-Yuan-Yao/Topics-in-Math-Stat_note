[["index.html", "Topics in Mathematical Statistics Chapter 1 Introduction", " Topics in Mathematical Statistics Huei-Yuan Yao 2022-09-27 Chapter 1 Introduction This is a lecture notes of Topics in Mathematical Statistics, 2021 Fall and 2022 Spring in NCCU, lectured by Professor Tzee-Ming Huang. The textbook we used is Mathematical Statistics by Jun Shao. The goal of this lecture note is to: Review what I have learned in mathematical statistics. Teach myself something new (at least to me) in the book. Help myself (or perhaps others) understand statistics more rigorously. First, we will introduce measure-based probability which may take readers a lot of time if they are not familiar with concepts in real analysis or measure theory. Next, we will discuss statistical decision theory that unify statistical inference such as point estimators and hypothesis tests. Last but not least we will go through some estimation techniques in parametric and nonparametric models. A point of this class is to teach us how to prove efficiency and asymptotic normality rigorously so readers may find these two chapters more detailed. Obviously, there are lots of things left in the book but I think what we have known is enough to comprehend the remains. However, I will learn and add some topics that not appeared in class if it is adequate. "],["prob.html", "Chapter 2 Measure-based Probability theory", " Chapter 2 Measure-based Probability theory In this chapter, we quickly review the concepts in measure-based probability theory. The measure theory help us rigorously and axiomatically describe the randomness. When it comes to talk about a probability of an event, first we have to know about the underlying probability space. "],["probability-spaces-and-random-elements.html", "2.1 Probability Spaces and Random Elements", " 2.1 Probability Spaces and Random Elements 2.1.1 Recap: Measure theory We start from some basic concepts in the classical measure theory and use them to describe the probability space. Definition 2.1 (Sigma Algebra (Field)) Let \\(\\cal F\\) be a collection of subsets of a sample space \\(\\Omega\\). \\(\\cal F\\) is called a sigma algebra on \\(\\Omega\\) if The empty set \\(\\phi \\in \\cal F\\). If \\(A \\in \\cal F\\), then \\(A^c \\in \\cal F\\). If \\(A_i \\in \\cal F\\) for \\(i=1,2,\\cdots\\), then \\(\\cup_{i=1}^{\\infty} A_i \\in \\cal F\\). We denote the smallest sigma algebra containing a collection \\(C\\) as \\(\\sigma(C)\\). In particular, we denote \\(\\cal B= \\sigma(C)\\) the sigma algebra (called Borel field) where \\(C\\) denotes all finite open interval on \\(\\mathbb{R}\\). By Linderbergs covering lemma, we can see \\(\\cal B\\) is also the smallest sigma algebra containing the collection of all open sets on \\(\\mathbb{R}\\). Furthermore, we denote \\(\\cal B^k\\) as the Borel field in \\(\\mathbb{R}^k\\). Define the pair \\((\\Omega, \\cal F)\\) a measurable space if \\(\\cal F\\) is a sigma algebra on \\(\\Omega\\). Definition 2.2 (Measure) A set function \\(\\nu(\\cdot)\\) defined on \\(\\cal F\\) is a measure if \\(0 \\leq \\nu(A) \\leq \\infty\\) for any \\(A \\in \\cal F\\). \\(\\nu(\\phi)=0\\) If \\(A_i \\in \\cal F\\), \\(i=1,2,\\cdots\\) and \\(A_i\\)s are disjoint for any \\(i\\neq j\\), then \\[\\nu(\\cup_i A_i)=\\sum_i \\nu(A_i)\\] The triple \\((\\Omega, \\cal F, \\nu)\\) is called a measure space and it is called a probability space if \\(\\nu(\\Omega)=1\\). Proposition 2.1 Let \\((\\Omega,\\cal F,\\nu)\\) be a measure space. If \\(A \\subset B\\), then \\(\\nu(A)\\leq \\nu (B)\\). For any sequence \\(A_1,A_2,\\cdots\\), \\[\\nu(\\cup_i A_i)\\leq \\sum_i \\nu(A_i).\\] If \\(A_1 \\subset A_2 \\subset \\cdots\\) (or \\(A_1 \\supset A_2 \\supset \\cdots\\) and \\(\\nu(A_k)&lt;\\infty\\) for some \\(k\\in \\mathbb{N}\\)), then \\[\\nu(\\lim_{k \\to \\infty}A_k)=\\lim_{k\\to \\infty}\\nu(A_k).\\] Below we see some uniqueness theorem about the measure, or the result of the well known Dinkins \\(\\pi\\)-\\(\\lambda\\) theorem. Definition 2.3 A collection \\(C\\) of some subsets of \\(\\Omega\\) is a \\(\\pi\\)-system if it is closed under intersection. Definition 2.4 Let \\(C\\) be a collection of some subsets of \\(\\Omega\\). A measure \\(\\mu\\) is \\(\\sigma\\)-finite on \\(C\\) if there exist a sequence of sets \\(\\{A_k\\}\\) in \\(C\\) such that \\(\\Omega=\\cup_k A_k\\) and \\(\\mu(A_K)&lt;\\infty\\) for all \\(k\\). Lebesgue measure is an example of \\(\\sigma\\)-finite measure on \\(\\cal B\\). Since probability measure is always finite, so it is \\(sigma\\)-finite. Theorem 2.1 (Theorem 10.3 in Billingsley, 1986) Suppose that \\(\\mu_1\\) and \\(\\mu_2\\) are measures on \\(\\sigma(\\cal P)\\), where \\(\\cal P\\) is a \\(\\pi\\)-system. Suppose they are both \\(\\sigma\\)-finite and agree on \\(\\cal P\\), then they also agree on \\(\\sigma(\\cal P)\\). 2.1.2 Measurable Functions Lemma 2.1 (Theorem A.42 in Schervish (1995)) Let \\(Y\\) be measurable from \\((\\Omega,\\cal F)\\) to \\((\\Lambda_Y,\\cal G_Y)\\) and \\(Z\\) be measurable from \\((\\Omega,\\cal F)\\) to \\((\\Lambda_Z,\\cal G_Z)\\). Define \\(T=\\{Y(\\omega),\\omega \\in \\Omega\\} \\subset \\Lambda_Y\\). Then \\(Z\\) is measurable from \\((\\Omega,\\sigma(Y))\\) to \\((\\Lambda_Z,\\cal G_Z)\\) if and only if \\(Z=h \\circ Y\\) for some \\(h\\) that is measurable from \\((T,T\\cap \\cal G_Y)\\) to \\((\\Lambda_Z,\\cal G_Z)\\). Remark: Proof of the if side is relatively obvious. (WLOG, we can assume \\(\\Lambda_Y\\) is the range of \\(Y\\).) Proof (only if). At first, we show that for \\(\\omega_1,\\ \\omega_2 \\in \\Omega\\), \\(Y(\\omega_1)=Y(\\omega_2)\\) implies \\(Z(\\omega_1)=Z(\\omega_2)\\). Suppose that \\(Y(\\omega_1)=Y(\\omega_1)=a\\). Since \\(Z\\) is measurable with respect to \\(\\sigma(Y)\\), there exist \\(A \\in \\cal G_Y\\) such that \\(Y^{-1}(A) =Z^{-1}(\\{Z(\\omega_1)\\})\\). Clearly \\(\\omega_1,\\ \\omega_2 \\in Z^{-1}(\\{Z(\\omega_1)\\})\\), thus \\(Z(\\omega_1)=Z(\\omega_2)\\). By this first step, we can well defined the function \\(h\\) such that \\(h(Y(\\omega))=Z(\\omega)\\). Secondly, we prove such \\(h\\) is measurable. "],["integration-and-differentiation.html", "2.2 Integration and Differentiation", " 2.2 Integration and Differentiation "],["conditional-expectation.html", "2.3 Conditional Expectation", " 2.3 Conditional Expectation "],["asymptotical-theory.html", "2.4 Asymptotical Theory", " 2.4 Asymptotical Theory "],["statistical-decision-theory.html", "Chapter 3 Statistical Decision Theory ", " Chapter 3 Statistical Decision Theory "],["general-framework.html", "3.1 General Framework", " 3.1 General Framework "],["point-estimators.html", "3.2 Point Estimators", " 3.2 Point Estimators 3.2.1 UMVUE 3.2.2 U-Statistics 3.2.3 V-Statistics "],["hypothesis-tests.html", "3.3 Hypothesis Tests", " 3.3 Hypothesis Tests 3.3.1 UMP Tests 3.3.2 Tests in Parametric Models 3.3.3 Tests in Nonparametric Models "],["confidence-sets.html", "3.4 Confidence Sets", " 3.4 Confidence Sets "],["estimation-in-parametric-models.html", "Chapter 4 Estimation in Parametric Models ", " Chapter 4 Estimation in Parametric Models "],["bayes-estimators.html", "4.1 Bayes Estimators", " 4.1 Bayes Estimators 4.1.1 Markov chain Monte Carlo 4.1.2 Asymptotic efficiency of Bayes Estimators "],["method-of-maximum-likelihood.html", "4.2 Method of Maximum Likelihood", " 4.2 Method of Maximum Likelihood 4.2.1 MLE 4.2.2 Quasi-likelihoods and conditional likelihoods 4.2.3 Asymptotic efficiency We describe our methods in this chapter. Math can be added in body using usual syntax like this "],["math-example.html", "4.3 math example", " 4.3 math example \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\) \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] "],["estimation-in-nonparametric-models.html", "Chapter 5 Estimation in Nonparametric Models ", " Chapter 5 Estimation in Nonparametric Models "],["distributional-estimators.html", "5.1 Distributional Estimators", " 5.1 Distributional Estimators "],["statistical-functionals.html", "5.2 Statistical Functionals", " 5.2 Statistical Functionals "],["sample-quantiles.html", "5.3 Sample quantiles", " 5.3 Sample quantiles "],["generalized-estimating-equations.html", "5.4 Generalized Estimating Equations", " 5.4 Generalized Estimating Equations 5.4.1 Framework 5.4.2 Consistency 5.4.3 Asymptotic normality "],["generalized-methods-of-moments.html", "5.5 Generalized Methods of Moments", " 5.5 Generalized Methods of Moments "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "]]
